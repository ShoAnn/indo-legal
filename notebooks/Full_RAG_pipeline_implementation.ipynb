{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "krZ-pD36yV02"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth_zoo unsloth vllm\n",
        "!pip install bitsandbytes datasets rank_bm25 datasets scikit-learn gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Jb5E8HUPX-ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84154681-9749-4d46-cd70-3a000a07855e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-20 08:36:21 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 05-20 08:36:21 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Document Corpus"
      ],
      "metadata": {
        "id": "Bd4UXU_dQ858"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lb_all = load_dataset(\"ShoAnn/indonesian-criminal-law-statutory\")\n",
        "\n",
        "corpus = [f\"{lb_all['train']['text'][i]} >> {lb_all['train']['fulltext'][i]}\" for i in range(len(lb_all['train']))]"
      ],
      "metadata": {
        "id": "qsL66YQfyJs6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sparse_index():\n",
        "    print(\"Starting TF-IDF vectorization (indexing)...\")\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(lowercase=True)\n",
        "\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "    print(\"TF-IDF indexing finished.\")\n",
        "\n",
        "    print(\"Starting BM25 indexing...\")\n",
        "    tokenized_corpus_bm25 = [doc.lower().split() for doc in corpus]\n",
        "    bm25 = BM25Okapi(tokenized_corpus_bm25)\n",
        "    print(\"BM25 index built.\")\n",
        "    return tfidf_vectorizer, tfidf_matrix, bm25\n",
        "\n",
        "def prepare_model_and_embedding():\n",
        "\n",
        "    base_model = SentenceTransformer(\"indobenchmark/indobert-base-p1\")\n",
        "    model = SentenceTransformer(\"ShoAnn/indobert-base-p1-legalqa-retriever\")\n",
        "    corpus_embeddings_base = base_model.encode(corpus, convert_to_tensor=True)\n",
        "    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "    return base_model, model, corpus_embeddings_base, corpus_embeddings\n",
        "\n",
        "def load_unsloth_model(model_name, max_seq_length=2048, load_in_4bit=True, load_in_8bit=False, full_finetuning=False):\n",
        "    \"\"\"Loads a model using the unsloth library.\"\"\"\n",
        "    if \"gemma\" in model_name:\n",
        "        from unsloth import FastModel\n",
        "        model_loader = FastModel.from_pretrained\n",
        "    else:\n",
        "        model_loader = FastLanguageModel.from_pretrained\n",
        "\n",
        "    model, tokenizer = model_loader(\n",
        "        model_name = model_name,\n",
        "        max_seq_length = max_seq_length,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        load_in_8bit = load_in_8bit,\n",
        "        full_finetuning = full_finetuning,\n",
        "    )\n",
        "    from unsloth.chat_templates import get_chat_template\n",
        "    if \"gemma\" in model_name:\n",
        "        tokenizer = get_chat_template(\n",
        "            tokenizer,\n",
        "            chat_template = \"gemma-3\",\n",
        "        )\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_transformers_model(base_model_id, model_id=\"\", peft=False):\n",
        "    \"\"\"Loads a model using the transformers library, with optional PEFT.\"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
        "\n",
        "    if peft:\n",
        "        model = PeftModel.from_pretrained(base_model, model_id)\n",
        "    else:\n",
        "        model = base_model\n",
        "\n",
        "    model.eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model(name):\n",
        "    if \"unsloth\" in name:\n",
        "        if \"legalqa\" in name:\n",
        "            model, tokenizer = load_unsloth_model(f\"ShoAnn/{name}\")\n",
        "        else:\n",
        "            model, tokenizer = load_unsloth_model(f\"unsloth/{name}\")\n",
        "    else:\n",
        "        if \"finetuned\" in name:\n",
        "            base_model_list = [\"SeaLLMs/SeaLLMs-v3-7B-Chat\", \"aisingapore/Llama-SEA-LION-v3.5-8B-R\"]\n",
        "            for base_model in base_model_list:\n",
        "                if name in base_model:\n",
        "                    model, tokenizer = load_transformers_model(base_model, \"ShoAnn/\"+name, peft=True)\n",
        "        else:\n",
        "            model, tokenizer = load_transformers_model(name, peft=True)\n",
        "    return model, tokenizer\n",
        "\n",
        "instruction_prompt = \"You are an AI Legal Assistant. Your task is to carefully analyze the provided **Legal Basis** below and answer given **Question** based *solely* and *exclusively* on the information contained within that context in Indonesian Language.\"\n",
        "def format_to_message(question, input):\n",
        "    message = []\n",
        "    message.append(\n",
        "        {'content': [{\n",
        "            'type': 'text',\n",
        "            'text': f'{instruction_prompt} \\n **Question** \\n{question} \\n**Legal Basis** \\n{input}'\n",
        "        }], 'role': 'user'},\n",
        "    )\n",
        "    return message\n",
        "\n",
        "retriever_model_list = [\n",
        "    \"TF-IDF\",\n",
        "    \"BM25\",\n",
        "    \"indobenchmark/indobert-base-p1\",\n",
        "    \"ShoAnn/indobert-base-p1-legalqa-retriever\",\n",
        "]\n",
        "\n",
        "generator_model_list = [\n",
        "    \"SeaLLMs-v3-7B-Chat\",\n",
        "    \"Llama-SEA-LION-v3.5-8B-R\",\n",
        "    \"gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    \"SeaLLMs-v3-7B-Chat-legalqa\",\n",
        "    \"Llama-SEA-LION-v3.5-8B-R-legalqa\",\n",
        "    \"gemma-3-4b-it-unsloth-bnb-4bit-legalqa\",\n",
        "    \"Qwen3-8B-unsloth-bnb-4bit-legalqa\"\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "qsfnngA8Q7wd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model, model, corpus_embeddings_base, corpus_embeddings = prepare_model_and_embedding()\n",
        "tfidf_vectorizer, tfidf_matrix, bm25 = build_sparse_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBc-RccJOZy2",
        "outputId": "eb088be7-1f44-4f15-9bb3-7002ac59d1e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting TF-IDF vectorization (indexing)...\n",
            "TF-IDF matrix shape: (5474, 13320)\n",
            "TF-IDF indexing finished.\n",
            "Starting BM25 indexing...\n",
            "BM25 index built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models"
      ],
      "metadata": {
        "id": "EbXZXVnQLjgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. RAG Core Logic ---\n",
        "\n",
        "def retrieve_documents(query_text, retriever=\"indobert\", k=5):\n",
        "    \"\"\"\n",
        "    Retrieves the top k most relevant documents for a given query\n",
        "    using simple cosine similarity search over a list of embeddings.\n",
        "    \"\"\"\n",
        "    if not query_text or not corpus:\n",
        "        return []\n",
        "    if \"bert\" in retriever.lower():\n",
        "        encoder = model if \"legalqa\" in retriever else base_model\n",
        "        doc_embeddings = corpus_embeddings if \"legalqa\" in retriever else corpus_embeddings_base\n",
        "        query_embedding = encoder.encode([query_text], convert_to_tensor=True) # Shape: (1, embedding_dim)\n",
        "    else:\n",
        "        doc_embeddings = tfidf_matrix\n",
        "        query_embedding = tfidf_vectorizer.transform([query_text]) # Shape: (1, num_features)\n",
        "\n",
        "    # Calculate cosine similarities between query embedding and all document embeddings\n",
        "    similarity_scores = model.similarity(query_embedding, corpus_embeddings)[0]\n",
        "    scores, indices = torch.topk(similarity_scores, k=k)\n",
        "    retrieved_docs = []\n",
        "    for score, idx in zip(scores, indices):\n",
        "        retrieved_docs.append((corpus[idx], score.detach().cpu().item()))\n",
        "    return retrieved_docs\n",
        "\n",
        "\n",
        "def rag_app_interface(user_query, selected_retriever_model, selected_generator_model_name, num_docs_to_retrieve):\n",
        "    if not user_query:\n",
        "        return \"Please enter a query.\", \"\", \"\" # Matches 3 outputs\n",
        "\n",
        "    # 1. Retrieve relevant documents\n",
        "    retrieved_docs_with_scores = retrieve_documents(user_query, selected_retriever_model, k=int(num_docs_to_retrieve))\n",
        "\n",
        "    # Format retrieved documents for display and for prompt\n",
        "    formatted_retrieved_docs_display = \"## Retrieved Documents:\\n\\n\"\n",
        "    context_for_prompt = \"\"\n",
        "    if retrieved_docs_with_scores:\n",
        "        for i, (doc_text, doc_score) in enumerate(retrieved_docs_with_scores):\n",
        "            formatted_retrieved_docs_display += f\"**Document {i+1} (Score: {doc_score:.4f}):**\\n{doc_text.strip()}\\n\\n\"\n",
        "            context_for_prompt += f\"Document {i+1}:\\n{doc_text.strip()}\\n\\n\"\n",
        "    else:\n",
        "        formatted_retrieved_docs_display += \"No relevant documents found.\"\n",
        "        context_for_prompt = \"No relevant documents found.\"\n",
        "\n",
        "\n",
        "    generator_llm, generator_tokenizer = load_model(selected_generator_model_name)\n",
        "\n",
        "    # Example of creating a prompt string (adjust as needed for your specific model/template)\n",
        "    # This is often model-specific. Some models use special tokens.\n",
        "    # Using apply_chat_template is generally preferred if available and correctly used.\n",
        "    # For this example, assuming format_to_message gives a list of dicts for chat template:\n",
        "    # chat_messages = [\n",
        "    #    {\"role\": \"user\", \"content\": f\"Based on the following documents, answer the question.\\n\\nDocuments:\\n{context_for_prompt}\\n\\nQuestion: {user_query}\"}\n",
        "    # ]\n",
        "    # If format_to_message creates this list:\n",
        "    chat_messages = format_to_message(user_query, context_for_prompt)\n",
        "\n",
        "\n",
        "    # `apply_chat_template` typically returns a string if tokenize=False\n",
        "    formatted_prompt_str = generator_tokenizer.apply_chat_template(\n",
        "        chat_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True # Adds the prompt for the assistant to start generating\n",
        "    )\n",
        "\n",
        "    # 3. Generate an answer\n",
        "    inputs = generator_tokenizer([formatted_prompt_str], return_tensors=\"pt\").to(generator_llm.device)\n",
        "    input_ids_length = inputs.input_ids.shape[1]\n",
        "\n",
        "    # Ensure pad_token_id is set for open-ended generation if not already handled in load_model\n",
        "    if generator_tokenizer.pad_token_id is None:\n",
        "        generator_tokenizer.pad_token_id = generator_tokenizer.eos_token_id\n",
        "\n",
        "    with torch.no_grad(): # Important for inference\n",
        "        outputs_tokens = generator_llm.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=1.0,\n",
        "                    top_p=0.95,\n",
        "                    top_k=64,\n",
        "                    pad_token_id=generator_tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "    # Decode only the newly generated tokens\n",
        "    generated_ids = outputs_tokens[0][input_ids_length:]\n",
        "    final_answer_text = generator_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # The Gradio interface expects 3 outputs\n",
        "    return formatted_retrieved_docs_display, formatted_prompt_str, final_answer_text\n",
        "\n",
        "# --- 4. Gradio UI Definition ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# RAG Legal Chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            query_input = gr.Textbox(label=\"Pertanyaan:\")\n",
        "            retriever_selector = gr.Dropdown(\n",
        "                label=\"Pilih Model Retriever:\",\n",
        "                choices=retriever_model_list,\n",
        "                value=retriever_model_list[0]\n",
        "            )\n",
        "            generator_selector = gr.Dropdown(\n",
        "                label=\"Pilih Model Generator:\",\n",
        "                choices=generator_model_list,\n",
        "                value=generator_model_list[0]\n",
        "            )\n",
        "            docs_slider = gr.Slider(\n",
        "                minimum=1, maximum=5, value=3, step=1,\n",
        "                label=\"Jumlah Dokumen yang Ingin Diambil:\"\n",
        "            )\n",
        "            submit_button = gr.Button(\"Submit\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            retrieved_docs_output = gr.Markdown(label=\"Retrieved Documents\")\n",
        "            full_prompt_output = gr.Markdown(label=\"Full Prompt to Generator\")\n",
        "            answer_output = gr.Markdown(label=\"Generated Answer\")\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=rag_app_interface,\n",
        "        inputs=[query_input, retriever_selector, generator_selector, docs_slider],\n",
        "        outputs=[retrieved_docs_output, full_prompt_output, answer_output]\n",
        "    )\n",
        "\n",
        "# --- 5. Launch the App ---\n",
        "# For Colab, share=True is often useful to get a public link.\n",
        "# For local use, you can omit share=True or set it to False.\n",
        "print(\"Launching Gradio app...\")\n",
        "demo.launch(share=True, debug=True) # debug=True can be helpful during development"
      ],
      "metadata": {
        "id": "2OsW5tuwKZMv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "5be24eae-0522-4b49-d59c-01f51c11c648"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Gradio app...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6fb7a49d0dafa29e37.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6fb7a49d0dafa29e37.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.6: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6fb7a49d0dafa29e37.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}