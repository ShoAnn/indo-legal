{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22733,"status":"ok","timestamp":1740110078817,"user":{"displayName":"M. Shokhibul Anwar","userId":"05956381970651564421"},"user_tz":-420},"id":"Tj3DaSQEldu7","outputId":"897674f3-60ef-4da9-f453-d24a9fef7450"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m163.8/209.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rank_bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\n","Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Installing collected packages: rank_bm25\n","Successfully installed rank_bm25-0.2.2\n"]}],"source":["import pandas as pd\n","import ast\n","\n","import nltk\n","from tqdm import tqdm\n","import re\n","import math\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer  # You can keep PorterStemmer for English parts of text\n","\n","!pip install -q sastrawi\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # For Indonesian Stemmer\n","from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n","nltk.download('punkt_tab', quiet=True)\n","\n","!pip install rank_bm25"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcIpQn-UiE6j"},"outputs":[],"source":["d_df = pd.read_csv(\"/content/drive/MyDrive/SKRIPSI/lb_v1_5474.csv\", index_col=0)\n","q_df = pd.read_csv(\"/content/drive/MyDrive/SKRIPSI/main_v1_1118.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyvWv6s84MGq"},"outputs":[],"source":["\n","class BM25:\n","    def __init__(self, documents, k1=1.2, b=0.75):\n","        self.documents = documents\n","        self.k1 = k1\n","        self.b = b\n","        self.N = len(documents)\n","        self.stemmer_factory = StemmerFactory() #Sastrawi Stemmer Factory\n","        self.stemmer_id = self.stemmer_factory.create_stemmer() #Create Sastrawi Stemmer\n","        self.stop_word_remover_factory = StopWordRemoverFactory() #Sastrawi Stopword Factory\n","        self.stop_words_id = self.stop_word_remover_factory.create_stop_word_remover() #Create Sastrawi Stopwords\n","        self.avgdl = sum(len(self._tokenize(d)) for d in documents) / self.N\n","        self._initialize_idfs()\n","        self.stemmer_en = PorterStemmer()  # Keep PorterStemmer if you still need it\n","\n","    def _len_docs(self):\n","        return len(self.documents)\n","\n","    def _tokenize(self, text):\n","        # 1. Case folding:\n","        text = text.lower()\n","\n","        # 2. Special character cleaning (more comprehensive):\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","\n","        # 3. Tokenization:\n","        tokens = word_tokenize(text)\n","\n","        # 4. Stop word removal (Indonesian):\n","        tokens = [w for w in tokens if not self.stop_words_id.remove(w)] #Use Sastrawi Stopwords\n","\n","        # 5. Stemming (Indonesian):\n","        stemmed_tokens = [self.stemmer_id.stem(token) for token in tokens] #Use Sastrawi Stemmer\n","        return stemmed_tokens\n","\n","    def _initialize_idfs(self):\n","        self.idfs = {}\n","        for document in tqdm(self.documents):\n","            tokens = self._tokenize(document)\n","            for term in set(tokens):\n","                if term not in self.idfs:\n","                    n_qi = sum(1 for d in self.documents if term in self._tokenize(d))\n","                    self.idfs[term] = math.log((self.N - n_qi + 0.5) / (n_qi + 0.5)) if n_qi > 0 else 0.0\n","\n","    def get_scores(self, query):\n","        scores = []\n","        tokenized_query = self._tokenize(query)  # Tokenize the query\n","        doc_id_text = {i+1: d for i, d in enumerate(self.documents)}\n","        for i in doc_id_text:\n","            tokens = self._tokenize(doc_id_text[i]) # Tokenize the document\n","            score = 0\n","            for term in tokenized_query:\n","                if term in self.idfs:\n","                    tf = tokens.count(term)\n","                    idf = self.idfs[term]\n","                    score += idf * (tf / (self.k1 + tf)) * ((1 - self.b) + self.b * (len(tokens) / self.avgdl))\n","            scores.append({\"doc_id\": i, \"score\": score})\n","        return scores\n","\n","    def get_ranked_documents(self, query):\n","        scores = self.get_scores(query)\n","        # Sort by the 'score' key within each dictionary in scores\n","        ranked_documents = sorted(scores, key=lambda x: x['score'], reverse=True)\n","        return ranked_documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CrZTG3rjDyV"},"outputs":[],"source":["import json\n","import os\n","import signal\n","from tqdm import tqdm\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from functools import lru_cache\n","from typing import Dict, List, Any, Tuple\n","import numpy as np\n","\n","class Miner:\n","    \"\"\"\n","    Optimized mining class with checkpointing capabilities and progress tracking\n","\n","    Attributes:\n","        q_data: dictionary of q_id: q_text\n","        q_rel: dictionary of q_id: [doc_ids]\n","        d_data: dictionary of doc_id: doc_text\n","        checkpoint_file: path to save checkpoints\n","        checkpoint_frequency: save progress every N steps\n","        batch_size: number of queries to process in parallel\n","        cache_size: size of LRU cache for document ranking\n","    \"\"\"\n","    def __init__(\n","        self,\n","        q_data: Dict[str, str],\n","        q_rel: Dict[str, List[str]],\n","        d_data: Dict[str, str],\n","        checkpoint_file: str = \"mining_checkpoint.json\",\n","        checkpoint_frequency: int = 100,\n","        batch_size: int = 4,\n","        cache_size: int = 1000\n","    ):\n","        self.q_data = q_data\n","        self.q_rel = q_rel\n","        self.d_data = d_data\n","        self.checkpoint_file = checkpoint_file\n","        self.checkpoint_frequency = checkpoint_frequency\n","        self.batch_size = batch_size\n","        self.cache_size = cache_size\n","\n","        # Pre-initialize containers for better performance\n","        self.output: List[Dict[str, str]] = []\n","        self.interrupted = False\n","        self.last_processed_qid = None\n","\n","        # Convert document data to numpy arrays for faster processing\n","        self.doc_ids = np.array(list(self.d_data.keys()))\n","        self.doc_texts = np.array(list(self.d_data.values()))\n","\n","        # Initialize thread pool\n","        self.executor = ThreadPoolExecutor(max_workers=self.batch_size)\n","\n","        # Register signal handlers\n","        signal.signal(signal.SIGINT, self._handle_interrupt)\n","        signal.signal(signal.SIGTERM, self._handle_interrupt)\n","\n","        # Load previous checkpoint if exists\n","        self._load_checkpoint()\n","\n","        # Initialize ranker once\n","        print(\"Initializing global ranker...\")\n","        docs_tuple = tuple(self.d_data.values())\n","        self.global_ranker = self._initiate_ranker(docs_tuple)\n","        print(\"Global ranker initialized successfully\")\n","\n","    def _handle_interrupt(self, signum, frame):\n","        \"\"\"Handle interrupt signals by setting flag and saving checkpoint\"\"\"\n","        print(\"\\nInterrupt received. Saving checkpoint and stopping gracefully...\")\n","        self.interrupted = True\n","        self._save_checkpoint()\n","        self.executor.shutdown(wait=True)\n","\n","    def _save_checkpoint(self):\n","        \"\"\"Save current progress to checkpoint file using numpy for faster serialization\"\"\"\n","        checkpoint_data = {\n","            'output': self.output,\n","            'last_processed_qid': self.last_processed_qid\n","        }\n","        np.save(self.checkpoint_file, checkpoint_data)\n","        print(f\"\\nCheckpoint saved to {self.checkpoint_file}\")\n","\n","    def _load_checkpoint(self):\n","        \"\"\"Load progress from checkpoint file if it exists\"\"\"\n","        checkpoint_path = f\"{self.checkpoint_file}.npy\"\n","        if os.path.exists(checkpoint_path):\n","            try:\n","                checkpoint_data = np.load(checkpoint_path, allow_pickle=True).item()\n","                self.output = checkpoint_data.get('output', [])\n","                self.last_processed_qid = checkpoint_data.get('last_processed_qid')\n","                print(f\"Loaded checkpoint with {len(self.output)} processed items\")\n","            except Exception as e:\n","                print(f\"Error loading checkpoint: {e}\")\n","                self.output = []\n","                self.last_processed_qid = None\n","\n","    @lru_cache(maxsize=1000)\n","    def _initiate_ranker(self, docs: Tuple[str, ...]) -> Any:\n","        \"\"\"Cached ranker initialization with hashable input\"\"\"\n","        return BM25(list(docs))\n","\n","    def _process_query(self, qid: str, pos_pbar: tqdm = None) -> List[Dict[str, str]]:\n","        \"\"\"Process a single query and return mining results\"\"\"\n","        results = []\n","        pos_ids = self.q_rel[qid]\n","\n","        # Get scores using vectorized operations\n","        rel_docs = self.global_ranker.get_ranked_documents(self.q_data[qid])\n","        neg_candidates = np.array([doc[\"doc_id\"] for doc in rel_docs if doc[\"doc_id\"] not in pos_ids])\n","\n","        if len(neg_candidates) < 5:\n","            if pos_pbar:\n","                pos_pbar.update(len(pos_ids))\n","            return results\n","\n","        # Get top 5 candidates efficiently\n","        top_candidates = neg_candidates[:5]\n","        candidates_text = self.doc_texts[np.isin(self.doc_ids, top_candidates)]\n","        neg_ranker = self._initiate_ranker(tuple(candidates_text))\n","\n","        # Process positives in parallel\n","        futures = []\n","        for pos in pos_ids:\n","            futures.append(\n","                self.executor.submit(\n","                    self._process_positive,\n","                    pos=pos,\n","                    pos_text=self.d_data[pos],\n","                    neg_ranker=neg_ranker,\n","                    neg_candidates=top_candidates,\n","                    qid=qid\n","                )\n","            )\n","\n","        # Collect results\n","        for future in futures:\n","            result = future.result()\n","            if result:\n","                results.append(result)\n","            if pos_pbar:\n","                pos_pbar.update(1)\n","\n","        return results\n","\n","    def _process_positive(self, pos: str, pos_text: str, neg_ranker: Any,\n","                         neg_candidates: np.ndarray, qid: str) -> Dict[str, str]:\n","        \"\"\"Process a single positive example\"\"\"\n","        ranked_negs = neg_ranker.get_ranked_documents(pos_text)\n","        best_candidate = ranked_negs[0][\"doc_id\"]\n","        real_neg_id = neg_candidates[best_candidate]\n","        return {\"qid\": qid, \"pos\": pos, \"neg\": real_neg_id}\n","\n","    def mine(self) -> List[Dict[str, str]]:\n","        \"\"\"\n","        Mine data with improved performance through batching and parallel processing\n","        Returns:\n","            List of dictionaries containing mining results\n","        \"\"\"\n","        # Get list of qids to process, skipping already processed ones\n","        qids_to_process = list(self.q_rel.keys())\n","        if self.last_processed_qid and self.last_processed_qid in qids_to_process:\n","            start_idx = qids_to_process.index(self.last_processed_qid) + 1\n","            qids_to_process = qids_to_process[start_idx:]\n","\n","        # Calculate total positives for progress tracking\n","        total_positives = sum(len(self.q_rel[qid]) for qid in qids_to_process)\n","\n","        # Main progress bar for queries\n","        main_pbar = tqdm(\n","            total=len(qids_to_process),\n","            desc=\"Processing queries\",\n","            position=0\n","        )\n","\n","        # Progress bar for positive examples\n","        pos_pbar = tqdm(\n","            total=total_positives,\n","            desc=\"Processing positives\",\n","            position=1,\n","            leave=True\n","        )\n","\n","        # Process queries in batches\n","        for i in range(0, len(qids_to_process), self.batch_size):\n","            if self.interrupted:\n","                break\n","\n","            batch_qids = qids_to_process[i:i + self.batch_size]\n","\n","            # Process batch in parallel\n","            futures = [\n","                self.executor.submit(self._process_query, qid, pos_pbar)\n","                for qid in batch_qids\n","            ]\n","\n","            # Collect results\n","            for future in as_completed(futures):\n","                self.output.extend(future.result())\n","                main_pbar.update(1)\n","\n","            # Save checkpoint periodically\n","            if (i + self.batch_size) % self.checkpoint_frequency == 0:\n","                self.last_processed_qid = batch_qids[-1]\n","                self._save_checkpoint()\n","\n","        # Close progress bars\n","        main_pbar.close()\n","        pos_pbar.close()\n","\n","        # Final checkpoint and cleanup\n","        if not self.interrupted:\n","            self.last_processed_qid = qids_to_process[-1] if qids_to_process else None\n","            self._save_checkpoint()\n","            # Clean up checkpoint file if processing completed successfully\n","            checkpoint_path = f\"{self.checkpoint_file}.npy\"\n","            if os.path.exists(checkpoint_path):\n","                os.remove(checkpoint_path)\n","\n","        # Cleanup\n","        self.executor.shutdown(wait=True)\n","\n","        return self.output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFmOqF89jegM","outputId":"6c81e434-4467-450c-9fa0-96178e23f9a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing global ranker...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5474/5474 [06:28<00:00, 14.10it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Global ranker initialized successfully\n"]},{"output_type":"stream","name":"stderr","text":["Processing queries:   0%|          | 0/1118 [00:00<?, ?it/s]\n","Processing positives:   0%|          | 0/7037 [00:00<?, ?it/s]\u001b[A\n","\n","  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","\n","\n","  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n"," 20%|██        | 1/5 [00:51<03:26, 51.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 1/5 [00:52<03:28, 52.05s/it]\u001b[A\u001b[A\u001b[A\n","\n"," 20%|██        | 1/5 [00:52<03:29, 52.26s/it]\u001b[A\u001b[A\n","\n","\n","\n","\n"," 20%|██        | 1/5 [00:51<03:27, 51.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n"," 40%|████      | 2/5 [00:58<01:15, 25.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n"," 60%|██████    | 3/5 [00:58<00:27, 13.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n"," 80%|████████  | 4/5 [00:59<00:08,  8.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","100%|██████████| 5/5 [00:59<00:00, 11.88s/it]\n","\n","\n"," 40%|████      | 2/5 [00:59<01:17, 25.76s/it]\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 2/5 [00:59<01:17, 25.69s/it]\u001b[A\u001b[A\u001b[A\n","\n"," 60%|██████    | 3/5 [00:59<00:28, 14.07s/it]\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 3/5 [00:59<00:28, 14.04s/it]\u001b[A\u001b[A\u001b[A\n","\n"," 80%|████████  | 4/5 [00:59<00:08,  8.57s/it]\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 4/5 [00:59<00:08,  8.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n"," 40%|████      | 2/5 [00:59<01:16, 25.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 5/5 [00:59<00:00, 11.97s/it]\n","\n","\n","\n","\n","\n"," 60%|██████    | 3/5 [00:59<00:28, 14.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","100%|██████████| 5/5 [00:59<00:00, 11.90s/it]\n","\n","\n","100%|██████████| 5/5 [01:00<00:00, 12.12s/it]\n"]}],"source":["def get_rel_docs(qid):\n","    raw_docs = q_df.loc[qid, \"context\"]\n","    docs = ast.literal_eval(raw_docs)\n","    output = []\n","    for doc in docs:\n","        output.append(doc)\n","    return output\n","\n","q_data = {qid: q_df.loc[qid, \"question\"] for qid in q_df.index}\n","q_rel = {qid: get_rel_docs(qid) for qid in q_df.index}\n","d_data = {doc_id: d_df.loc[doc_id, \"fulltext\"] for doc_id in d_df.index}\n","\n","ranker = Miner(\n","    q_data=q_data,\n","    q_rel=q_rel,\n","    d_data=d_data,\n","    checkpoint_file=\"/content/drive/MyDrive/SKRIPSI/mining_checkpoint_cached.json\",  # optional\n","    checkpoint_frequency=25,  # optional\n","    batch_size=4,\n","    cache_size=1_000_000\n",")\n","q_pos_neg = ranker.mine()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lS78q_6p8eDr"},"outputs":[],"source":["q_pos_neg_df = pd.DataFrame(q_pos_neg)\n","q_pos_neg_df.to_csv(\"/content/drive/MyDrive/SKRIPSI/q_pos_neg.csv\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"10cJ4mS20tTdqhlESLj9cAcblaKvLhF4K","timestamp":1740105395031}],"mount_file_id":"1wrmsrNqS-sC5zCt6-G5pUFCQOQlJ9KRd","authorship_tag":"ABX9TyMYp4hamwGRh12kJUCwVb2d"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}