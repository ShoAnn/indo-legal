{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %%capture\n!pip install torch~=2.6.0 'torch_xla[tpu]~=2.6.0' \\\n  -f https://storage.googleapis.com/libtpu-releases/index.html \\\n  -f https://storage.googleapis.com/libtpu-wheels/index.html\n!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes -q\n!pip3 install accelerate datasets sentencepiece langchain -q\n!pip uninstall -y tensorflow\n!pip install tensorflow-cpu -q\n!git clone https://github.com/IsNoobgrammer/Pytorch-Optimizers optims","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:06:52.058814Z","iopub.execute_input":"2025-05-13T23:06:52.059130Z","iopub.status.idle":"2025-05-13T23:08:52.487541Z","shell.execute_reply.started":"2025-05-13T23:06:52.059100Z","shell.execute_reply":"2025-05-13T23:08:52.483064Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://storage.googleapis.com/libtpu-releases/index.html, https://storage.googleapis.com/libtpu-wheels/index.html\nRequirement already satisfied: torch~=2.6.0 in /usr/local/lib/python3.10/site-packages (2.6.0)\nRequirement already satisfied: torch_xla[tpu]~=2.6.0 in /usr/local/lib/python3.10/site-packages (2.6.0+libtpu)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (9.1.0.70)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (2.21.5)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (12.4.127)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (12.4.5.8)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (4.13.2)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (11.6.1.9)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (3.1.6)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (1.13.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (2025.3.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (3.18.0)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (12.4.127)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (3.4.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (12.4.127)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (0.6.2)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (10.3.5.147)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (12.4.127)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (12.4.127)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (11.2.1.3)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/site-packages (from torch~=2.6.0) (3.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch~=2.6.0) (1.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torch_xla[tpu]~=2.6.0) (2.32.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from torch_xla[tpu]~=2.6.0) (6.0.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torch_xla[tpu]~=2.6.0) (2.0.2)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from torch_xla[tpu]~=2.6.0) (2.2.2)\nCollecting libtpu==0.0.7.1\n  Downloading libtpu-0.0.7.1-py3-none-manylinux_2_27_x86_64.whl (131.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting libtpu-nightly==0.1.dev20241010+nightly.cleanup\n  Downloading https://storage.googleapis.com/libtpu-nightly-releases/wheels/libtpu-nightly/libtpu_nightly-0.1.dev20241010%2Bnightly.cleanup-py3-none-any.whl (1.3 kB)\nRequirement already satisfied: tpu-info in /usr/local/lib/python3.10/site-packages (from torch_xla[tpu]~=2.6.0) (0.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch~=2.6.0) (3.0.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla[tpu]~=2.6.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla[tpu]~=2.6.0) (2.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla[tpu]~=2.6.0) (3.4.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla[tpu]~=2.6.0) (2025.4.26)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from tpu-info->torch_xla[tpu]~=2.6.0) (14.0.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/site-packages (from tpu-info->torch_xla[tpu]~=2.6.0) (5.29.4)\nRequirement already satisfied: grpcio>=1.65.5 in /usr/local/lib/python3.10/site-packages (from tpu-info->torch_xla[tpu]~=2.6.0) (1.72.0rc1)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->tpu-info->torch_xla[tpu]~=2.6.0) (2.19.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->tpu-info->torch_xla[tpu]~=2.6.0) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->tpu-info->torch_xla[tpu]~=2.6.0) (0.1.2)\nInstalling collected packages: libtpu-nightly, libtpu\n  Attempting uninstall: libtpu-nightly\n    Found existing installation: libtpu-nightly 0.1.dev20241002+nightly\n    Uninstalling libtpu-nightly-0.1.dev20241002+nightly:\n      Successfully uninstalled libtpu-nightly-0.1.dev20241002+nightly\n  Attempting uninstall: libtpu\n    Found existing installation: libtpu 2.18.0\n    Uninstalling libtpu-2.18.0:\n      Successfully uninstalled libtpu-2.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-tpu 2.18.0 requires libtpu==2.18.0, but you have libtpu 0.0.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed libtpu-0.0.7.1 libtpu-nightly-0.1.dev20241010+nightly.cleanup\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-tpu 2.18.0 requires libtpu==2.18.0, but you have libtpu 0.0.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nFound existing installation: tensorflow 2.18.1\nUninstalling tensorflow-2.18.1:\n  Successfully uninstalled tensorflow-2.18.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-tpu 2.18.0 requires libtpu==2.18.0, but you have libtpu 0.0.7.1 which is incompatible.\ntensorflow-tpu 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\ntensorflow-tpu 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nfatal: destination path 'optims' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#get_ipython().kernel.do_shutdown(True)\n### for good measures restart kernel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:08:52.488666Z","iopub.execute_input":"2025-05-13T23:08:52.488915Z","iopub.status.idle":"2025-05-13T23:08:52.502692Z","shell.execute_reply.started":"2025-05-13T23:08:52.488886Z","shell.execute_reply":"2025-05-13T23:08:52.497339Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Sharding Module for different Architechture**","metadata":{}},{"cell_type":"code","source":"%%writefile spmd_util.py\nimport math\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\nfrom collections import defaultdict\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.distributed.spmd.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, MT5Config, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config, MixtralConfig, PhiConfig,GemmaConfig\n)\n\n# ends with $ to prevent sharding lora parameters\n\nMT5_RULES = (\n    # embeddings\n    (\"shared$\", (\"mp\", \"fsdp\")),\n    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"q$\", (\"fsdp\", \"mp\")),\n    (\"k$\", (\"fsdp\", \"mp\")),\n    (\"v$\", (\"fsdp\", \"mp\")),\n    (\"o$\", (\"mp\", \"fsdp\")),\n    \n    # mlp\n    (\"wi$\", (\"fsdp\", \"mp\")),\n    (\"wi_0$\", (\"fsdp\", \"mp\")),\n    (\"wi_1$\", (\"fsdp\", \"mp\")),\n    (\"wo$\", (\"mp\", \"fsdp\")),\n    \n    # output head\n    (\"lm_head$\", (\"fsdp\", \"mp\")),\n    (\"final_layer_norm$\", (\"fsdp\", \"mp\")),\n)\n\nT5_RULES = (\n    # embeddings\n    (\"shared$\", (\"mp\", \"fsdp\")),\n    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"q$\", (\"fsdp\", \"mp\")),\n    (\"k$\", (\"fsdp\", \"mp\")),\n    (\"v$\", (\"fsdp\", \"mp\")),\n    (\"o$\", (\"mp\", \"fsdp\")),\n\n    # mlp\n    (\"w$\", (\"fsdp\", \"mp\")),\n    (\"wi_0$\", (\"fsdp\", \"mp\")),\n    (\"wi_1$\", (\"fsdp\", \"mp\")),\n    (\"wo$\", (\"mp\", \"fsdp\")),\n\n    # seq2seq lm head\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n\nQWEN_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\nGPT2_RULES = (\n    # embeddings\n    (\"wte\", (\"mp\", \"fsdp\")), \n    (\"wpe\", (\"mp\", \"fsdp\")),\n    \n    # attention\n    (\"c_attn\", (\"fsdp\", \"mp\")),\n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # mlp\n    (\"c_fc\", (\"fsdp\", \"mp\")), \n    (\"c_proj\", (\"mp\", \"fsdp\")),\n    \n    # output \n    (\"ln_f\", (\"fsdp\", \"mp\")),\n)\nMISTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.(gate_proj|up_proj)\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\n\nPHI_RULES = (\n    ### (regex) linear modules, (list[sharding methods]) )\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.dense\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.fc2\", (\"mp\", \"fsdp\")),  \n    (\"mlp\\\\.fc1\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    \n)\n\nLLAMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\nGPTNEOX_RULES = (\n    # embeddings\n    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n    # atention\n    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n    # mlp\n    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n    # output\n    (\"embed_out\", (\"fsdp\", \"mp\")),\n)\n\n\n\nMIXTRAL_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"w1\", (\"fsdp\", \"mp\")),\n    (\"w2\", (\"mp\", \"fsdp\")),\n    (\"w3\", (\"fsdp\", \"mp\")),\n    (\"gate\", (\"mp\", \"fsdp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n    )\n\nGEMMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n    \nALL_RULES = [\n    (GPTNeoXConfig, GPTNEOX_RULES),\n    (MT5Config, MT5_RULES),\n    (T5Config, T5_RULES),\n    (LlamaConfig, LLAMA_RULES),\n    (GPT2Config, GPT2_RULES),\n    (MistralConfig, MISTRAL_RULES),\n    (Qwen2Config, QWEN_RULES),\n    (MixtralConfig, MIXTRAL_RULES),\n    (PhiConfig,PHI_RULES),\n    (GemmaConfig,GEMMA_RULES),\n]\n\n\ndef find_rule(model):\n    for config, rule in ALL_RULES:\n        x1=(str(config).split(\".\"))[-1]\n        x2=(str(model.config.__class__).split(\".\"))[-1]\n#         print(x1,x2)\n        if x1.lower()==x2.lower():\n            return rule\n    raise Exception(\"unsupported model to partitioning\")\n\nstrkey2id = {\n    \"dp\": 0, ## usefull for sharding inputs\n    \"fsdp\": 1, ## Pytorch-Xla (2D-sharding) axis to shard data (mostly mesh shape will be (8,1)) data will be sharded 8 way \n    \"mp\": 2 ## axis to shard model model will be sharded one way \n               ## Recommened checking Pytorch-tpu/transfomers on github (xla-fork of transformers)\n}\n\ndef partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n    partition_specs = find_rule(model)\n    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n        \n    # print(rule)\n\n    for name, module in model.named_modules():\n        module.to(device)\n#         print(name, module.__class__.__name__)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            for rule_pattern, spec in rule:\n                if re.findall(rule_pattern, name.lower())  : # and (\"lora\" not in name.lower()):\n                    if verbose:\n                        print(\"match\", rule_pattern, name)\n                    \n                    xs.mark_sharding(module.weight, mesh, spec)\n                    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:08:52.504454Z","iopub.execute_input":"2025-05-13T23:08:52.504686Z","iopub.status.idle":"2025-05-13T23:08:52.524620Z","shell.execute_reply.started":"2025-05-13T23:08:52.504662Z","shell.execute_reply":"2025-05-13T23:08:52.519430Z"}},"outputs":[{"name":"stdout","text":"Overwriting spmd_util.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**Required Libs**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch\nimport torch.nn as nn\nimport torch_xla.test.test_utils as test_utils\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, set_seed, DataCollatorWithPadding, AutoConfig \n)\n\nfrom transformers import logging as hf_logging\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.distributed.spmd.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model \nfrom datasets import  load_dataset, concatenate_datasets\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset as TorchDataset\nfrom torch_xla.utils.checkpoint import checkpoint\n\ntry:\n    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n    os.environ.pop('TPU_PROCESS_ADDRESSES')\n    os.environ.pop('CLOUD_TPU_TASK_ID')\n    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:08:52.525343Z","iopub.execute_input":"2025-05-13T23:08:52.525575Z","iopub.status.idle":"2025-05-13T23:09:48.415675Z","shell.execute_reply.started":"2025-05-13T23:08:52.525554Z","shell.execute_reply":"2025-05-13T23:09:48.409119Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"MODEL = \"aisingapore/Llama-SEA-LION-v3.5-8B-R\"#You should be able to use 7B model with no changes! There should be enough HBM\nSAVED_MODEL = f\"{MODEL.split('/')[1]}-legalqa\"\nWANDB_PROJECT = MODEL.split('/')[1]\n\n# !export XLA_TENSOR_ALLOCATOR_MAXSIZE=1000000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:48.418247Z","iopub.execute_input":"2025-05-13T23:09:48.419269Z","iopub.status.idle":"2025-05-13T23:09:48.432819Z","shell.execute_reply.started":"2025-05-13T23:09:48.419238Z","shell.execute_reply":"2025-05-13T23:09:48.426273Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nif 'pad_token' not in tokenizer.special_tokens_map:\n  tokenizer.pad_token=tokenizer.eos_token\n\n\nprint(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:48.433607Z","iopub.execute_input":"2025-05-13T23:09:48.433858Z","iopub.status.idle":"2025-05-13T23:09:50.352996Z","shell.execute_reply.started":"2025-05-13T23:09:48.433834Z","shell.execute_reply":"2025-05-13T23:09:50.348055Z"}},"outputs":[{"name":"stdout","text":"Tokens :\n {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<|finetune_right_pad_id|>'} \n\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dataset = load_dataset(\"ShoAnn/legalqa_klinik_hukumonline\")\ntrain_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n\n# Split the test_valid into test and validation\ntest_valid = dataset[\"test\"]\n\nfrom datasets import DatasetDict\n# Gather the splits into a DatasetDict\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"test\": train_val[\"test\"],\n    \"valid\": test_valid\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:50.355541Z","iopub.execute_input":"2025-05-13T23:09:50.355836Z","iopub.status.idle":"2025-05-13T23:09:53.217350Z","shell.execute_reply.started":"2025-05-13T23:09:50.355808Z","shell.execute_reply":"2025-05-13T23:09:53.211218Z"}},"outputs":[{"name":"stderr","text":"Generating train split: 100%|██████████| 1006/1006 [00:00<00:00, 15611.48 examples/s]\nGenerating test split: 100%|██████████| 112/112 [00:00<00:00, 15214.97 examples/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"prompt_template = \"\"\"Below is a question paired with a context to the problem in the question. Write an answer that appropriately answers the question based on the context.\n\n### Question:\n{}\n\n### Context:\n{}\n\n### Answer:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"question\"]\n    inputs       = examples[\"context\"]\n    outputs      = examples[\"answer\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = prompt_template.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:53.219291Z","iopub.execute_input":"2025-05-13T23:09:53.219527Z","iopub.status.idle":"2025-05-13T23:09:53.592463Z","shell.execute_reply.started":"2025-05-13T23:09:53.219503Z","shell.execute_reply":"2025-05-13T23:09:53.586179Z"}},"outputs":[{"name":"stderr","text":"Map: 100%|██████████| 905/905 [00:00<00:00, 3255.36 examples/s]\nMap: 100%|██████████| 101/101 [00:00<00:00, 3435.03 examples/s]\nMap: 100%|██████████| 112/112 [00:00<00:00, 3471.05 examples/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class ConversationDataset(TorchDataset):\n    def __init__(self, tokenizer, max_length=1024, dataset=None):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        text = self.dataset[idx][\"text\"]\n        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n        return {\n            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n            \"attention_mask\":input_ids[\"attention_mask\"].squeeze(0),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:53.593977Z","iopub.execute_input":"2025-05-13T23:09:53.594454Z","iopub.status.idle":"2025-05-13T23:09:53.604915Z","shell.execute_reply.started":"2025-05-13T23:09:53.594425Z","shell.execute_reply":"2025-05-13T23:09:53.600453Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"len(dataset[\"train\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:53.608311Z","iopub.execute_input":"2025-05-13T23:09:53.608575Z","iopub.status.idle":"2025-05-13T23:09:53.680393Z","shell.execute_reply.started":"2025-05-13T23:09:53.608550Z","shell.execute_reply":"2025-05-13T23:09:53.675535Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"905"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': 2048,\n         'LOGGING_STEPS': 1,\n         'NUM_EPOCHS': 10,\n         'PAUSE_STEPS':1000, # asks to exit training after x steps , #todo checkpoint saving me no lazy\n         'MAX_STEPS': -1,#Ooverides num epochs\n         'BATCH_SIZE': 2, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n         'LEN_TRAIN_DATA': dataset[\"train\"],\n         'VAL_STEPS': 20,\n         'VAL_BATCH': 5,\n         'GRAD_ACCUMULATION_STEP':4,\n         'MAX_GRAD_CLIP':1,\n        'LEARNING_RATE':2e-5,\n         'WARMUP_RATIO':0.01,\n         'OPTIMIZER':'adamw', # default = 'adamw'  options->  ['adamw','SM3','came','adafactor','lion']           \n         'SCHEDULAR':'cosine', # default= 'cosine'     options:-> ['linear','cosine']\n         'WEIGHT_DECAY':0.1,\n         'TRAIN_DATASET':dataset[\"train\"],\n         \"TEST_DATASET\":dataset[\"train\"],\n         'WANDB':True,\n        'PROJECT': WANDB_PROJECT,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:53.683175Z","iopub.execute_input":"2025-05-13T23:09:53.683535Z","iopub.status.idle":"2025-05-13T23:09:53.694751Z","shell.execute_reply.started":"2025-05-13T23:09:53.683510Z","shell.execute_reply":"2025-05-13T23:09:53.689908Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**Quantization When??**","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16) \n# model._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint) \n# gradient checkpointing is not properly setup needs to do barieer optimization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:09:53.697650Z","iopub.execute_input":"2025-05-13T23:09:53.698195Z","iopub.status.idle":"2025-05-13T23:10:15.615799Z","shell.execute_reply.started":"2025-05-13T23:09:53.698170Z","shell.execute_reply":"2025-05-13T23:10:15.609635Z"}},"outputs":[{"name":"stderr","text":"Fetching 4 files: 100%|██████████| 4/4 [00:20<00:00,  5.14s/it]\nLoading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 49.23it/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**LoRA Applicable**","metadata":{}},{"cell_type":"code","source":"ls=LoraConfig(\n    r = 32, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n    target_modules =['q_proj', 'down_proj', 'up_proj', 'o_proj', 'v_proj', 'gate_proj', 'k_proj'],\n    lora_alpha = 16, #weight_scaling\n    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimize\n    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n)\nmodel = get_peft_model(model, ls)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:15.617214Z","iopub.execute_input":"2025-05-13T23:10:15.617484Z","iopub.status.idle":"2025-05-13T23:10:16.967768Z","shell.execute_reply.started":"2025-05-13T23:10:15.617456Z","shell.execute_reply":"2025-05-13T23:10:16.963826Z"}},"outputs":[{"name":"stderr","text":"WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"**Data-Distributer**","metadata":{}},{"cell_type":"code","source":"train_data = ConversationDataset(tokenizer, dataset=dataset[\"train\"] , max_length=FLAGS['MAX_INPUT'])\nval = ConversationDataset(tokenizer, dataset=dataset[\"valid\"])\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\ntraining_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\nval_sampler = torch.utils.data.distributed.DistributedSampler(\n    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\ntesting_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n\nprint(f\"Max Steps:- {len(training_loader)}  , eFFECTIVE bATCH size {8*FLAGS['BATCH_SIZE']} Input\")\nprint(f\"Val Size:- {len(testing_loader)}  , eFFECTIvE bATCH size {8*FLAGS['BATCH_SIZE']} Input\")\nFLAGS['STEPS']=len(training_loader)\nFLAGS['BATCH_DATA']=FLAGS['BATCH_SIZE']*8 ## 8 CORES ON TPU \n# print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:16.971543Z","iopub.execute_input":"2025-05-13T23:10:16.973273Z","iopub.status.idle":"2025-05-13T23:10:20.917679Z","shell.execute_reply.started":"2025-05-13T23:10:16.973241Z","shell.execute_reply":"2025-05-13T23:10:20.913853Z"}},"outputs":[{"name":"stderr","text":"WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n","output_type":"stream"},{"name":"stdout","text":"Max Steps:- 57  , eFFECTIVE bATCH size 16 Input\nVal Size:- 7  , eFFECTIvE bATCH size 16 Input\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(val[0]['input_ids'])\nc=0\nfor step, batch in enumerate(training_loader):\n\n    print(step)\n    print(tokenizer.decode(batch['input_ids'][0]))\n    break\n# print(tokenizer.decode(val[0]['input_ids']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:20.921101Z","iopub.execute_input":"2025-05-13T23:10:20.921444Z","iopub.status.idle":"2025-05-13T23:10:21.172510Z","shell.execute_reply.started":"2025-05-13T23:10:20.921417Z","shell.execute_reply":"2025-05-13T23:10:21.168573Z"}},"outputs":[{"name":"stdout","text":"tensor([128000,  39314,    374,  ...,  68531,   1141,  10602])\n0\n<|begin_of_text|>Below is a question paired with a context to the problem in the question. Write an answer that appropriately answers the question based on the context.\n\n### Question:\nKerabat saya ditahan polisi sebagai jaminan untuk teman dia yang melarikan diri, tetapi setelah sampai di Polsek yang bersangkutan, kerabat saya yang tidak melakukan apapun malah mendapat tindakan yang tidak menyenangkan yaitu penyiksaan. Bagaimana hukumnya ini?\n\n### Context:\n[{'full_text': '(2) Uang yang dimaksud dalam ayat (1) harus disetor ke Kas Negara melalui panitera pengadilan negeri.', 'id': 2562, 'name': 'Pasal 36 ayat (2) PP Pelaksanaan KUHAP'}, {'full_text': '8.  pengaduan masyarakat yang selanjutnya disebut dumas  adalah bentuk penerapan dari pengawasan masyarakat  yang disampaikan oleh masyarakat, instansi pemerintah  atau pihak lain kepada polri berupa sumbangan pikiran,  saran, gagasan atau keluhan/pengaduan yang bersifat  membangun', 'id': 2563, 'name': 'Pasal 1 angka 8 Perkap 9/2018'}, {'full_text': 'pasal 5   dumas  sebagaimana  dimaksud  dalam  pasal 4   dapat  disampaikan terkait dengan:  a.  pelayanan polri;  b.  penyimpangan  perilaku  pegawai  negeri  pada polri;  dan/atau  c.  penyalahgunaan wewenang.  bab ii  penatausahaan dumas', 'id': 2564, 'name': 'Pasal 5 Perkap 9/2018'}]\n\n### Answer:\nULASAN LENGKAP  Jaminan Orang untuk Penangguhan Penahanan Berdasarkan kronologi kasus yang Anda sampaikan, kami simpulkan bahwa kerabat Anda ditahan dan mengalami penyiksaan oleh oknum kepolisian, sebagai jaminan bagi temannya yang telah melarikan diri. Dalam hal ini kami asumsikan bahwa teman dari kerabat Anda tersebut ada pada proses penangguhan penahanan. Di dalam hukum pidana dikenal adanya jaminan orang, yang diberikan kepada tersangka yang meminta agar ia tidak ditempatkan dalam rumah tahanan selama proses penyidikan, penuntutan dan persidangan. Dalam Pasal 31 KUHAP dinyatakan: Atas permintaan tersangka atau terdakwa, penyidik atau penuntut umum atau hakim, sesuai dengan kewenangan masing-masing, dapat mengadakan penangguhan penahanan dengan atau tanpa jaminan uang atau jaminan orang, berdasarkan syarat yang ditentukan. Berdasarkan kronologi yang disampaikan, kerabat Anda yang dijadikan jaminan bagi temannya yang melarikan diri dalam hal ini dikategorikan sebagai jaminan orang. Sebagaimana dijelaskan dalam Syarat-syarat Penangguhan Penahanan, syarat menjadi jaminan orang adalah: Orang penjamin bisa penasihat hukumnya, keluarganya, atau orang lain yang tidak mempunyai hubungan apapun dengan tahanan; Penjamin memberikan pernyataan dan kepastian kepada instansi yang menahan bahwa ia bersedia dan bertanggung jawab memikul segala risiko dan akibat yang timbul apabila tahanan melarikan diri; Identitas orang yang menjamin harus disebutkan secara jelas; Instansi yang menahan menetapkan besarnya jumlah uang yang harus ditanggung oleh penjamin, yang disebut uang tanggungan (apabila tersangka/terdakwa) melarikan diri; Pengeluaran surat perintah penangguhan didasarkan atas surat jaminan dari si penjamin. Terkait perlakuan kekerasan oleh oknum kepolisian terhadap kerabat Anda, hal tersebut sungguh tidak dibenarkan oleh hukum. Karena pada dasarnya tidak ada aturan hukum manapun yang memperbolehkan tindakan penyiksaan maupun kekerasan. Larinya teman kerabat Anda tidak lantas membuat kerabat Anda dapat ditahan apalagi sampai menerima tindakan penganiayaan atau kekerasan dari oknum kepolisian, hal ini sebagaimana yang telah diatur secara tegas dalam Pasal 36 PP Pelaksanaan KUHAP yang menyatakan sebagai berikut: Dalam hal jaminan itu adalah orang, dan tersangka atau terdakwa melarikan diri maka setelah lewat waktu 3 (tiga) bulan tidak diketemukan, penjamin diwajibkan membayar uang yang jumlahnya telah ditetapkan oleh pejabat yang berwenang sesuai dengan tingkat pemeriksaan. Dengan demikian, kalau pun diasumsikan bahwa penangguhan penahanan dan penempatan jaminan orang dalam kasus ini sudah sesuai dengan syarat dan prosedur yang berlaku, bentuk pertanggungjawaban oleh kerabat Anda jika temannya melarikan diri setelah lewat waktu 3 bulan adalah membayar uang jaminan yang jumlahnya telah ditetapkan oleh pejabat yang berwenang sesuai dengan tingkat pemeriksaan, yang mana uang jaminan tersebut akan disetorkan oleh penjamin ke kas negara melalui Panitera Pengadilan Negeri. [1] Tindakan Kekerasan oleh Kepolisian Oknum kepolisian yang melakukan tindakan kekerasan terhadap kerabat Anda jelas telah melakukan suatu pelanggaran. Pada dasarnya, dalam melaksanakan tugas dan wewenangnya, pejabat Kepolisian Negara Republik Indonesia (Polri) senantiasa bertindak berdasarkan aturan hukum yang berlaku serta menjunjung tinggi Hak Asasi Manusia (HAM). Hal ini sebagaimana telah diatur dalam Pasal 19 UU Polri. Lebih lanjut dalam Pasal 10 huruf c Perkap 8/2009 disebutkan bahwa dalam melaksanakan tugas penegakan hukum, setiap petugas/anggota Polri wajib mematuhi ketentuan berperilaku (Code of Conduct) di antaranya tidak boleh menggunakan kekerasan, kecuali dibutuhkan untuk mencegah kejahatan membantu melakukan penangkapan terhadap pelanggar hukum atau tersangka sesuai dengan peraturan penggunaan kekerasan. Jika polisi harus melakukan tindakan kekerasan, maka tindakan tersebut harus mempertimbangkan hal-hal sebagaimana disebut dalam Pasal 45 Perkap 8/2009, yang di antaranya menyatakan bahwa, tindakan dan cara-cara tanpa kekerasan harus diusahakan terlebih dahulu, pun tindakan keras hanya diterapkan bila sangat diperlukan. Sehingga jelas bahwa tindakan penyiksaan yang dilakukan oleh oknum kepolisian telah melanggar aturan hukum dan Kode Etik Kepolisian yang berlaku. Terkait tindakan kekerasan yang dialami oleh kerabat, Anda dapat membuat upaya pengaduan masyarakat (“Dumas”), sebagaimana diatur dalam Perkap 9/2018. Dumas adalah bentuk penerapan dari pengawasan masyarakat yang disampaikan oleh masyarakat, Instansi Pemerintah atau pihak lain kepada Polri berupa sumbangan pikiran, saran, gagasan atau keluhan/pengaduan yang bersifat membangun. [2] Dumas dapat disampaikan terkait dengan: [3] Pelayanan Polri; Penyimpangan perilaku pegawai negeri pada Polri; dan/atau Penyalahgunaan wewenang. Proses Dumas dapat dilakukan secara daring melalui laman Dumas Presisi. Demikian jawaban dari kami, semoga bermanfaat. Dasar Hukum : Undang-Undang Nomor 8 Tahun 1981 tentang Hukum Acara Pidana ; Undang-Undang Nomor 2 Tahun 2002 tentang Kepolisian Negara Republik Indonesia ; Undang-Undang Nomor 11 Tahun 2020 tentang Cipta Kerja ; Peraturan Pemerintah Nomor 27 tahun 1983 tentang Pelaksanaan Kitab Undang-Undang Hukum Acara Pidana ; Peraturan Kepala Kepolisian Republik Indonesia Nomor 8 Tahun 2009\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def get_nb_trainable_parameters(model):\n        r\"\"\"\n        Returns the number of trainable parameters and number of all parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in model.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            # Due to the design of 4bit linear layers from bitsandbytes\n            # one needs to multiply the number of parameters by 2 to get\n            # the correct number of parameters\n            if param.__class__.__name__ == \"Params4bit\":\n                num_params = num_params * 2\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n\n        return trainable_params, all_param\ndef print_trainable_parameters(model):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params, all_param = get_nb_trainable_parameters(model)\n        \n        print(\n            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:21.175952Z","iopub.execute_input":"2025-05-13T23:10:21.177543Z","iopub.status.idle":"2025-05-13T23:10:21.265878Z","shell.execute_reply.started":"2025-05-13T23:10:21.177511Z","shell.execute_reply":"2025-05-13T23:10:21.262349Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:21.269370Z","iopub.execute_input":"2025-05-13T23:10:21.269607Z","iopub.status.idle":"2025-05-13T23:10:21.474574Z","shell.execute_reply.started":"2025-05-13T23:10:21.269583Z","shell.execute_reply":"2025-05-13T23:10:21.468264Z"}},"outputs":[{"name":"stdout","text":"trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338249554642545\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from spmd_util import partition_module\nimport torch_xla.distributed.parallel_loader as pl\n\ndevice = xm.xla_device()\nmodel = model.to(device)\nfrom torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear\nfrom torch_xla.distributed.spmd.xla_sharding import xla_patched_nn_linear_forward\nmodel = apply_xla_patch_to_nn_linear(model, xla_patched_nn_linear_forward)  #for patching linear layer to use einsum instead of matmul\nnum_devices = xr.global_runtime_device_count()\nmodel_axis=1\ndata_axis=num_devices//model_axis\nmesh_shape = (1,data_axis, model_axis )\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp','fsdp', 'mp'))\npartition_module(model, mesh)\ntraining_loader = pl.MpDeviceLoader(training_loader, device)\ntesting_loader = pl.MpDeviceLoader(testing_loader, device)\nmesh_shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:21.476553Z","iopub.execute_input":"2025-05-13T23:10:21.476819Z","iopub.status.idle":"2025-05-13T23:10:33.531725Z","shell.execute_reply.started":"2025-05-13T23:10:21.476792Z","shell.execute_reply":"2025-05-13T23:10:33.527294Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(1, 8, 1)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"FLAGS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:33.534142Z","iopub.execute_input":"2025-05-13T23:10:33.534396Z","iopub.status.idle":"2025-05-13T23:10:33.547610Z","shell.execute_reply.started":"2025-05-13T23:10:33.534372Z","shell.execute_reply":"2025-05-13T23:10:33.543391Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'MAX_INPUT': 2048,\n 'LOGGING_STEPS': 1,\n 'NUM_EPOCHS': 10,\n 'PAUSE_STEPS': 1000,\n 'MAX_STEPS': -1,\n 'BATCH_SIZE': 2,\n 'LEN_TRAIN_DATA': Dataset({\n     features: ['question', 'answer', 'context', 'text'],\n     num_rows: 905\n }),\n 'VAL_STEPS': 20,\n 'VAL_BATCH': 5,\n 'GRAD_ACCUMULATION_STEP': 4,\n 'MAX_GRAD_CLIP': 1,\n 'LEARNING_RATE': 2e-05,\n 'WARMUP_RATIO': 0.01,\n 'OPTIMIZER': 'adamw',\n 'SCHEDULAR': 'cosine',\n 'WEIGHT_DECAY': 0.1,\n 'TRAIN_DATASET': Dataset({\n     features: ['question', 'answer', 'context', 'text'],\n     num_rows: 905\n }),\n 'TEST_DATASET': Dataset({\n     features: ['question', 'answer', 'context', 'text'],\n     num_rows: 905\n }),\n 'WANDB': True,\n 'PROJECT': 'Llama-SEA-LION-v3.5-8B-R',\n 'STEPS': 57,\n 'BATCH_DATA': 16}"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"!export XLA_USE_BF16=1\nimport torch.nn as nn\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=wandb_key)\n__wandb__=FLAGS['WANDB']\nfrom torch_xla.amp.syncfree import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom optims.optim import SM3, CAME , Adafactor\nimport torch_xla.distributed.spmd as xs\n# from random import randrange\n# from bitsandbytes.optim import AdamW8bit \n# from torchdistx.optimizers import AnyPrecisionAdamW\n\nval_step=0\n\ndef evaluate_loss(outputs_logits, labels, pad_id=tokenizer.pad_token_id):\n    # Simplest possible loss for debugging XLA issues\n    # This bypasses all masking, gathering etc.\n    target_labels = labels[..., 1:].contiguous().long()\n    logits_for_loss = outputs_logits[..., :-1, :].contiguous().float()\n\n    # Ensure target_labels are within vocab size\n    vocab_size = logits_for_loss.size(-1)\n    target_labels = torch.clamp(target_labels, 0, vocab_size - 1)\n\n    loss = nn.functional.cross_entropy(\n        logits_for_loss.view(-1, vocab_size), \n        target_labels.view(-1),\n        ignore_index=pad_id # Use ignore_index if possible\n    )\n    return loss.to(outputs_logits.dtype)\n\n\n\ndef train(FLAGS):\n\n    ### Configuring Training\n    global val_step\n    update_params= filter(lambda p: p.requires_grad, model.parameters())\n    num_iterations = int((FLAGS[\"NUM_EPOCHS\"] * FLAGS['STEPS'] ) // FLAGS['GRAD_ACCUMULATION_STEP'])\n    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n    \n    if __wandb__:\n        wandb.init(project=FLAGS['PROJECT'],config=FLAGS)\n        wandb.define_metric(\"Validation_loss\", step_metric=\"val_step\")\n        wandb.define_metric(\"Learning_rate\",step_metric=\"train_step\")\n        wandb.define_metric(\"train_loss\",step_metric=\"train_step\")\n    \n    ### Optimizers\n    \n    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n        optimizer = Lion(update_params, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n        optimizer = Adafactor(update_params,lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=False,relative_step=False)\n    elif (FLAGS['OPTIMIZER']).lower()=='came':\n        optimizer = CAME(model.parameters(),lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],betas=(0.9, 0.999, 0.9999),eps=(1e-30, 1e-16))\n    else:\n#         optimizer = Lilith(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n        optimizer=SM3(update_params,lr=FLAGS['LEARNING_RATE'])\n\n    for param_group in optimizer.param_groups:\n        if len(param_group[\"params\"]) > 0:\n            print(param_group[\"params\"][0].device)\n            break\n    \n    ### Schedulars\n    \n    if (FLAGS['SCHEDULAR']).lower()=='linear':\n        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n    else:\n        scheduler = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n        \n    \n    ### Training Loop\n    val_step=0\n    check=False #for brakes\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        if check:\n            break\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        for step, batch in enumerate(training_loader):\n            input_ids, labels,attention_mask = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device),batch['attention_mask'].to(device)\n            xs.mark_sharding(input_ids, mesh, (0,1))  ### earlier:-> (0,1) according to pytorch-xla , input/dataloaders must be sharded across ('data',None) \n            xs.mark_sharding( labels,  mesh, (0,1))  ###\n            xs.mark_sharding(  attention_mask,  mesh, (0, 1)) ###\n        \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            # outputs.logits = outputs.logits.to(labels.dtype)  # ✅ Convert only logits\n            loss = evaluate_loss(outputs.logits, labels) # Pass the logits tensor\n\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                xm.master_print(f'loss: {loss.detach().cpu().item()}, time: {test_utils.now()}, step: {step+1}')\n            if __wandb__:\n                wandb.log({\n                'Learning_rate': optimizer.param_groups[0]['lr'],\n                'train_loss':  loss.detach().cpu().item(),\n                'train_step': step + 1 + ((epoch-1) * FLAGS[\"STEPS\"]),\n                })\n \n            del input_ids , attention_mask \n            loss.backward()\n            del outputs,loss\n            \n            if (step+1) % FLAGS['GRAD_ACCUMULATION_STEP'] == 0:\n                torch.nn.utils.clip_grad_norm_(update_params, max_norm=FLAGS['MAX_GRAD_CLIP']*8)\n                scheduler.step()\n                xm.optimizer_step(optimizer,pin_layout=True,barrier=True) ## performs xm.reduce_gradient() , optimizer.step() , xm.mark_step()\n                optimizer.zero_grad()\n            \n            if (step+1)% FLAGS['VAL_STEPS'] == 0:\n                end_index=FLAGS[\"VAL_BATCH\"]\n                model.eval()\n                with torch.no_grad():\n                    total_loss = 0\n                    total_step = 0\n                    for stepx, batchx in enumerate(testing_loader):\n                        input_ids = batchx[\"input_ids\"].to(device)\n                        labels = batchx[\"labels\"].to(device)\n                        attention_mask = batchx[\"attention_mask\"].to(device)\n                        xs.mark_sharding(input_ids, mesh, (0, None))\n                        xs.mark_sharding(labels, mesh, (0, None))\n                        xs.mark_sharding( attention_mask,    mesh, (0, None))\n                        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n                        # --->>> !!! THIS IS A DIFFERENT PROBLEM !!! <<<---\n                        loss = evaluate_loss(outputs.logits, labels)\n                        total_loss += loss.item()\n                        total_step +=1\n                        xm.master_print('----- Time -> {} ----- Validation Batch -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_step , loss.item()))\n                        if __wandb__:\n                            val_step+=1\n                            wandb.log({\n                                'Validation_loss': loss.item(),\n                                'val_step':val_step,\n                                    })\n                        if (stepx+1)%end_index==0:\n                            break\n                    model.train()    \n                    average_loss=total_loss/total_step\n                    xm.master_print('----- Time -> {} ----- Validation Batch Size -> {} ----  Validation Loss -> {:.7f}'.format(test_utils.now(), total_step , average_loss))\n\n            if (step+1)% FLAGS['PAUSE_STEPS']==0:\n                inp=input('want to continue training after {} steps'.format(step+1))\n                check = bool(\"no\" in inp.lower())\n                if check:\n                    break\n                else:\n                    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:10:33.549750Z","iopub.execute_input":"2025-05-13T23:10:33.549971Z","iopub.status.idle":"2025-05-13T23:11:00.498571Z","shell.execute_reply.started":"2025-05-13T23:10:33.549949Z","shell.execute_reply":"2025-05-13T23:11:00.494011Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshoann\u001b[0m (\u001b[33mshoann-mycompany-inc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**12 Mins to Train on 4k**","metadata":{}},{"cell_type":"code","source":"train(FLAGS)\nif FLAGS['WANDB']:\n    wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:11:00.500668Z","iopub.execute_input":"2025-05-13T23:11:00.501299Z","iopub.status.idle":"2025-05-13T23:57:27.398781Z","shell.execute_reply.started":"2025-05-13T23:11:00.501265Z","shell.execute_reply":"2025-05-13T23:57:27.394583Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"creating run (0.3s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.11"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250513_231100-z2g0lsdt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R/runs/z2g0lsdt' target=\"_blank\">snowy-shape-6</a></strong> to <a href='https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R' target=\"_blank\">https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R/runs/z2g0lsdt' target=\"_blank\">https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R/runs/z2g0lsdt</a>"},"metadata":{}},{"name":"stdout","text":"xla:0\nEpoch 1 train begin 23:11:03\nloss: 1.5390625, time: 23:11:43, step: 1\nloss: 1.296875, time: 23:13:51, step: 2\nloss: 1.25, time: 23:15:23, step: 3\nloss: 1.4765625, time: 23:15:26, step: 4\nloss: 1.125, time: 23:17:23, step: 5\nloss: 1.2890625, time: 23:18:49, step: 6\nloss: 1.1875, time: 23:18:51, step: 7\nloss: 1.375, time: 23:18:54, step: 8\nloss: 1.6484375, time: 23:20:58, step: 9\nloss: 1.0078125, time: 23:21:00, step: 10\nloss: 1.4375, time: 23:21:03, step: 11\nloss: 1.5703125, time: 23:21:06, step: 12\nloss: 1.40625, time: 23:21:09, step: 13\nloss: 1.2890625, time: 23:21:11, step: 14\nloss: 0.69140625, time: 23:21:14, step: 15\nloss: 1.484375, time: 23:21:16, step: 16\nloss: 1.1953125, time: 23:21:19, step: 17\nloss: 0.91796875, time: 23:21:22, step: 18\nloss: 1.2265625, time: 23:21:24, step: 19\nloss: 1.4609375, time: 23:21:27, step: 20\n----- Time -> 23:21:51 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6641\n----- Time -> 23:22:12 ----- Validation Batch -> 2 ----  Validation Loss -> 0.8828\n----- Time -> 23:22:12 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6016\n----- Time -> 23:22:13 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2031\n----- Time -> 23:22:14 ----- Validation Batch -> 5 ----  Validation Loss -> 1.4453\n----- Time -> 23:22:14 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3593750\nloss: 1.40625, time: 23:22:15, step: 21\nloss: 1.1796875, time: 23:22:17, step: 22\nloss: 1.1171875, time: 23:22:20, step: 23\nloss: 1.1171875, time: 23:22:22, step: 24\nloss: 1.328125, time: 23:22:26, step: 25\nloss: 1.4921875, time: 23:22:28, step: 26\nloss: 1.21875, time: 23:22:31, step: 27\nloss: 1.1484375, time: 23:22:33, step: 28\nloss: 1.40625, time: 23:22:36, step: 29\nloss: 1.1484375, time: 23:22:39, step: 30\nloss: 1.0859375, time: 23:22:41, step: 31\nloss: 1.265625, time: 23:22:44, step: 32\nloss: 1.4296875, time: 23:22:47, step: 33\nloss: 1.3828125, time: 23:22:50, step: 34\nloss: 1.0546875, time: 23:22:52, step: 35\nloss: 1.28125, time: 23:22:55, step: 36\nloss: 1.125, time: 23:22:58, step: 37\nloss: 1.4609375, time: 23:23:01, step: 38\nloss: 1.0546875, time: 23:23:03, step: 39\nloss: 1.0625, time: 23:23:06, step: 40\n----- Time -> 23:23:09 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6250\n----- Time -> 23:23:09 ----- Validation Batch -> 2 ----  Validation Loss -> 0.8555\n----- Time -> 23:23:10 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5703\n----- Time -> 23:23:10 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1719\n----- Time -> 23:23:11 ----- Validation Batch -> 5 ----  Validation Loss -> 1.4219\n----- Time -> 23:23:11 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3289062\nloss: 1.2265625, time: 23:23:12, step: 41\nloss: 1.3046875, time: 23:23:15, step: 42\nloss: 1.5, time: 23:23:17, step: 43\nloss: 0.6796875, time: 23:23:20, step: 44\nloss: 1.0078125, time: 23:23:23, step: 45\nloss: 1.15625, time: 23:23:26, step: 46\nloss: 1.0234375, time: 23:23:28, step: 47\nloss: 1.09375, time: 23:23:31, step: 48\nloss: 1.2265625, time: 23:23:34, step: 49\nloss: 1.5546875, time: 23:23:36, step: 50\nloss: 1.1875, time: 23:23:39, step: 51\nloss: 1.375, time: 23:23:42, step: 52\nloss: 1.125, time: 23:23:45, step: 53\nloss: 1.515625, time: 23:23:47, step: 54\nloss: 1.21875, time: 23:23:50, step: 55\nloss: 1.140625, time: 23:23:52, step: 56\nloss: 1.28125, time: 23:24:30, step: 57\nEpoch 2 train begin 23:26:02\nloss: 1.4375, time: 23:26:05, step: 1\nloss: 1.1875, time: 23:27:44, step: 2\nloss: 1.1640625, time: 23:27:46, step: 3\nloss: 1.3828125, time: 23:27:49, step: 4\nloss: 1.0625, time: 23:30:02, step: 5\nloss: 1.203125, time: 23:30:05, step: 6\nloss: 1.1171875, time: 23:30:07, step: 7\nloss: 1.28125, time: 23:30:10, step: 8\nloss: 1.5625, time: 23:30:14, step: 9\nloss: 0.91796875, time: 23:30:17, step: 10\nloss: 1.359375, time: 23:30:20, step: 11\nloss: 1.5, time: 23:30:22, step: 12\nloss: 1.328125, time: 23:30:25, step: 13\nloss: 1.2265625, time: 23:30:28, step: 14\nloss: 0.63671875, time: 23:30:30, step: 15\nloss: 1.40625, time: 23:30:33, step: 16\nloss: 1.109375, time: 23:30:36, step: 17\nloss: 0.85546875, time: 23:30:39, step: 18\nloss: 1.1484375, time: 23:30:41, step: 19\nloss: 1.3984375, time: 23:30:44, step: 20\n----- Time -> 23:30:47 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5547\n----- Time -> 23:30:48 ----- Validation Batch -> 2 ----  Validation Loss -> 0.8047\n----- Time -> 23:30:48 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5000\n----- Time -> 23:30:49 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1172\n----- Time -> 23:30:50 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3594\n----- Time -> 23:30:50 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2671875\nloss: 1.3203125, time: 23:30:51, step: 21\nloss: 1.109375, time: 23:30:53, step: 22\nloss: 1.0546875, time: 23:30:56, step: 23\nloss: 1.0390625, time: 23:30:59, step: 24\nloss: 1.25, time: 23:31:02, step: 25\nloss: 1.390625, time: 23:31:04, step: 26\nloss: 1.140625, time: 23:31:07, step: 27\nloss: 1.0859375, time: 23:31:09, step: 28\nloss: 1.3359375, time: 23:31:13, step: 29\nloss: 1.0703125, time: 23:31:15, step: 30\nloss: 1.0078125, time: 23:31:18, step: 31\nloss: 1.1875, time: 23:31:20, step: 32\nloss: 1.3515625, time: 23:31:23, step: 33\nloss: 1.296875, time: 23:31:26, step: 34\nloss: 0.98828125, time: 23:31:29, step: 35\nloss: 1.203125, time: 23:31:31, step: 36\nloss: 1.0546875, time: 23:31:34, step: 37\nloss: 1.390625, time: 23:31:37, step: 38\nloss: 0.97265625, time: 23:31:39, step: 39\nloss: 0.9921875, time: 23:31:42, step: 40\n----- Time -> 23:31:45 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5156\n----- Time -> 23:31:45 ----- Validation Batch -> 2 ----  Validation Loss -> 0.7773\n----- Time -> 23:31:46 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4609\n----- Time -> 23:31:47 ----- Validation Batch -> 4 ----  Validation Loss -> 1.0859\n----- Time -> 23:31:47 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3203\n----- Time -> 23:31:47 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2320312\nloss: 1.1640625, time: 23:31:48, step: 41\nloss: 1.21875, time: 23:31:51, step: 42\nloss: 1.421875, time: 23:31:54, step: 43\nloss: 0.62890625, time: 23:31:56, step: 44\nloss: 0.93359375, time: 23:31:59, step: 45\nloss: 1.078125, time: 23:32:02, step: 46\nloss: 0.95703125, time: 23:32:04, step: 47\nloss: 1.0234375, time: 23:32:07, step: 48\nloss: 1.15625, time: 23:32:10, step: 49\nloss: 1.484375, time: 23:32:13, step: 50\nloss: 1.125, time: 23:32:15, step: 51\nloss: 1.296875, time: 23:32:18, step: 52\nloss: 1.0703125, time: 23:32:21, step: 53\nloss: 1.4453125, time: 23:32:24, step: 54\nloss: 1.1484375, time: 23:32:26, step: 55\nloss: 1.078125, time: 23:32:29, step: 56\nloss: 1.1953125, time: 23:32:32, step: 57\nEpoch 3 train begin 23:32:35\nloss: 1.3671875, time: 23:32:37, step: 1\nloss: 1.125, time: 23:32:42, step: 2\nloss: 1.09375, time: 23:32:45, step: 3\nloss: 1.3203125, time: 23:32:47, step: 4\nloss: 1.0078125, time: 23:32:52, step: 5\nloss: 1.140625, time: 23:32:55, step: 6\nloss: 1.0546875, time: 23:32:57, step: 7\nloss: 1.1953125, time: 23:33:00, step: 8\nloss: 1.5, time: 23:33:04, step: 9\nloss: 0.87109375, time: 23:33:07, step: 10\nloss: 1.2890625, time: 23:33:10, step: 11\nloss: 1.421875, time: 23:33:12, step: 12\nloss: 1.265625, time: 23:33:15, step: 13\nloss: 1.15625, time: 23:33:18, step: 14\nloss: 0.59375, time: 23:33:20, step: 15\nloss: 1.34375, time: 23:33:23, step: 16\nloss: 1.046875, time: 23:33:26, step: 17\nloss: 0.80859375, time: 23:33:29, step: 18\nloss: 1.09375, time: 23:33:31, step: 19\nloss: 1.3359375, time: 23:33:34, step: 20\n----- Time -> 23:33:37 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4688\n----- Time -> 23:33:38 ----- Validation Batch -> 2 ----  Validation Loss -> 0.7305\n----- Time -> 23:33:38 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4062\n----- Time -> 23:33:39 ----- Validation Batch -> 4 ----  Validation Loss -> 1.0312\n----- Time -> 23:33:40 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2656\n----- Time -> 23:33:40 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.1804687\nloss: 1.25, time: 23:33:41, step: 21\nloss: 1.0546875, time: 23:33:43, step: 22\nloss: 1.0, time: 23:33:46, step: 23\nloss: 0.9921875, time: 23:33:48, step: 24\nloss: 1.1953125, time: 23:33:52, step: 25\nloss: 1.3203125, time: 23:33:54, step: 26\nloss: 1.078125, time: 23:33:57, step: 27\nloss: 1.0390625, time: 23:33:59, step: 28\nloss: 1.2890625, time: 23:34:02, step: 29\nloss: 1.015625, time: 23:34:05, step: 30\nloss: 0.953125, time: 23:34:07, step: 31\nloss: 1.1328125, time: 23:34:10, step: 32\nloss: 1.2890625, time: 23:34:13, step: 33\nloss: 1.2265625, time: 23:34:16, step: 34\nloss: 0.94921875, time: 23:34:18, step: 35\nloss: 1.1484375, time: 23:34:21, step: 36\nloss: 0.99609375, time: 23:34:24, step: 37\nloss: 1.3359375, time: 23:34:27, step: 38\nloss: 0.91015625, time: 23:34:29, step: 39\nloss: 0.94140625, time: 23:34:32, step: 40\n----- Time -> 23:34:34 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4453\n----- Time -> 23:34:35 ----- Validation Batch -> 2 ----  Validation Loss -> 0.7031\n----- Time -> 23:34:36 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3672\n----- Time -> 23:34:36 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9961\n----- Time -> 23:34:37 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2344\n----- Time -> 23:34:37 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.1492187\nloss: 1.1171875, time: 23:34:38, step: 41\nloss: 1.1484375, time: 23:34:41, step: 42\nloss: 1.3671875, time: 23:34:43, step: 43\nloss: 0.58984375, time: 23:34:46, step: 44\nloss: 0.87890625, time: 23:34:49, step: 45\nloss: 1.015625, time: 23:34:52, step: 46\nloss: 0.90625, time: 23:34:56, step: 47\nloss: 0.96875, time: 23:34:59, step: 48\nloss: 1.1015625, time: 23:35:02, step: 49\nloss: 1.4296875, time: 23:35:04, step: 50\nloss: 1.0703125, time: 23:35:07, step: 51\nloss: 1.234375, time: 23:35:10, step: 52\nloss: 1.0234375, time: 23:35:13, step: 53\nloss: 1.390625, time: 23:35:15, step: 54\nloss: 1.09375, time: 23:35:18, step: 55\nloss: 1.03125, time: 23:35:21, step: 56\nloss: 1.1171875, time: 23:35:24, step: 57\nEpoch 4 train begin 23:35:27\nloss: 1.3203125, time: 23:35:29, step: 1\nloss: 1.0703125, time: 23:35:34, step: 2\nloss: 1.0390625, time: 23:35:37, step: 3\nloss: 1.2734375, time: 23:35:39, step: 4\nloss: 0.94921875, time: 23:35:44, step: 5\nloss: 1.0859375, time: 23:35:47, step: 6\nloss: 0.9921875, time: 23:35:49, step: 7\nloss: 1.1328125, time: 23:35:52, step: 8\nloss: 1.453125, time: 23:35:56, step: 9\nloss: 0.8203125, time: 23:35:59, step: 10\nloss: 1.2265625, time: 23:36:02, step: 11\nloss: 1.3671875, time: 23:36:04, step: 12\nloss: 1.203125, time: 23:36:07, step: 13\nloss: 1.0859375, time: 23:36:10, step: 14\nloss: 0.5546875, time: 23:36:12, step: 15\nloss: 1.296875, time: 23:36:15, step: 16\nloss: 0.98828125, time: 23:36:18, step: 17\nloss: 0.75390625, time: 23:36:21, step: 18\nloss: 1.046875, time: 23:36:23, step: 19\nloss: 1.2734375, time: 23:36:26, step: 20\n----- Time -> 23:36:29 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3984\n----- Time -> 23:36:30 ----- Validation Batch -> 2 ----  Validation Loss -> 0.6602\n----- Time -> 23:36:31 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3125\n----- Time -> 23:36:31 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9414\n----- Time -> 23:36:32 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1797\n----- Time -> 23:36:32 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0984375\nloss: 1.171875, time: 23:36:33, step: 21\nloss: 0.9921875, time: 23:36:36, step: 22\nloss: 0.94921875, time: 23:36:38, step: 23\nloss: 0.94140625, time: 23:36:41, step: 24\nloss: 1.1328125, time: 23:36:44, step: 25\nloss: 1.2578125, time: 23:36:46, step: 26\nloss: 1.0234375, time: 23:36:49, step: 27\nloss: 1.0, time: 23:36:52, step: 28\nloss: 1.2421875, time: 23:36:55, step: 29\nloss: 0.96484375, time: 23:36:57, step: 30\nloss: 0.90234375, time: 23:37:00, step: 31\nloss: 1.0859375, time: 23:37:02, step: 32\nloss: 1.234375, time: 23:37:06, step: 33\nloss: 1.171875, time: 23:37:08, step: 34\nloss: 0.90234375, time: 23:37:11, step: 35\nloss: 1.109375, time: 23:37:13, step: 36\nloss: 0.953125, time: 23:37:16, step: 37\nloss: 1.28125, time: 23:37:19, step: 38\nloss: 0.859375, time: 23:37:22, step: 39\nloss: 0.89453125, time: 23:37:24, step: 40\n----- Time -> 23:37:27 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3750\n----- Time -> 23:37:28 ----- Validation Batch -> 2 ----  Validation Loss -> 0.6445\n----- Time -> 23:37:28 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2812\n----- Time -> 23:37:29 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9180\n----- Time -> 23:37:30 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1562\n----- Time -> 23:37:30 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0750000\nloss: 1.0703125, time: 23:37:31, step: 41\nloss: 1.078125, time: 23:37:33, step: 42\nloss: 1.3125, time: 23:37:36, step: 43\nloss: 0.5546875, time: 23:37:38, step: 44\nloss: 0.83203125, time: 23:37:42, step: 45\nloss: 0.96875, time: 23:37:44, step: 46\nloss: 0.86328125, time: 23:37:47, step: 47\nloss: 0.9296875, time: 23:37:49, step: 48\nloss: 1.046875, time: 23:37:52, step: 49\nloss: 1.3828125, time: 23:37:55, step: 50\nloss: 1.03125, time: 23:37:58, step: 51\nloss: 1.171875, time: 23:38:00, step: 52\nloss: 0.98046875, time: 23:38:03, step: 53\nloss: 1.3359375, time: 23:38:06, step: 54\nloss: 1.046875, time: 23:38:08, step: 55\nloss: 1.0, time: 23:38:11, step: 56\nloss: 1.0546875, time: 23:38:14, step: 57\nEpoch 5 train begin 23:38:17\nloss: 1.28125, time: 23:38:20, step: 1\nloss: 1.0234375, time: 23:38:25, step: 2\nloss: 0.99609375, time: 23:38:27, step: 3\nloss: 1.25, time: 23:38:30, step: 4\nloss: 0.91015625, time: 23:38:34, step: 5\nloss: 1.046875, time: 23:38:37, step: 6\nloss: 0.94140625, time: 23:38:40, step: 7\nloss: 1.078125, time: 23:38:42, step: 8\nloss: 1.4140625, time: 23:38:47, step: 9\nloss: 0.7890625, time: 23:38:49, step: 10\nloss: 1.1796875, time: 23:38:52, step: 11\nloss: 1.328125, time: 23:38:54, step: 12\nloss: 1.15625, time: 23:38:58, step: 13\nloss: 1.03125, time: 23:39:00, step: 14\nloss: 0.53125, time: 23:39:03, step: 15\nloss: 1.2578125, time: 23:39:05, step: 16\nloss: 0.953125, time: 23:39:08, step: 17\nloss: 0.72265625, time: 23:39:11, step: 18\nloss: 1.015625, time: 23:39:14, step: 19\nloss: 1.2265625, time: 23:39:16, step: 20\n----- Time -> 23:39:20 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3438\n----- Time -> 23:39:20 ----- Validation Batch -> 2 ----  Validation Loss -> 0.6250\n----- Time -> 23:39:21 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2500\n----- Time -> 23:39:21 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8867\n----- Time -> 23:39:22 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1250\n----- Time -> 23:39:22 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0460938\nloss: 1.125, time: 23:39:23, step: 21\nloss: 0.953125, time: 23:39:26, step: 22\nloss: 0.91015625, time: 23:39:28, step: 23\nloss: 0.91015625, time: 23:39:31, step: 24\nloss: 1.0859375, time: 23:39:34, step: 25\nloss: 1.2109375, time: 23:39:36, step: 26\nloss: 0.984375, time: 23:39:39, step: 27\nloss: 0.9765625, time: 23:39:42, step: 28\nloss: 1.21875, time: 23:39:45, step: 29\nloss: 0.9375, time: 23:39:47, step: 30\nloss: 0.87109375, time: 23:39:50, step: 31\nloss: 1.046875, time: 23:39:52, step: 32\nloss: 1.1953125, time: 23:39:56, step: 33\nloss: 1.1328125, time: 23:39:58, step: 34\nloss: 0.87890625, time: 23:40:01, step: 35\nloss: 1.078125, time: 23:40:03, step: 36\nloss: 0.92578125, time: 23:40:06, step: 37\nloss: 1.25, time: 23:40:09, step: 38\nloss: 0.828125, time: 23:40:12, step: 39\nloss: 0.8671875, time: 23:40:14, step: 40\n----- Time -> 23:40:17 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3438\n----- Time -> 23:40:18 ----- Validation Batch -> 2 ----  Validation Loss -> 0.6172\n----- Time -> 23:40:18 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2344\n----- Time -> 23:40:19 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8750\n----- Time -> 23:40:19 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1094\n----- Time -> 23:40:20 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0359375\nloss: 1.046875, time: 23:40:21, step: 41\nloss: 1.03125, time: 23:40:23, step: 42\nloss: 1.2890625, time: 23:40:26, step: 43\nloss: 0.53515625, time: 23:40:28, step: 44\nloss: 0.8046875, time: 23:40:31, step: 45\nloss: 0.9375, time: 23:40:34, step: 46\nloss: 0.83984375, time: 23:40:37, step: 47\nloss: 0.90625, time: 23:40:39, step: 48\nloss: 1.015625, time: 23:40:42, step: 49\nloss: 1.359375, time: 23:40:45, step: 50\nloss: 1.0, time: 23:40:47, step: 51\nloss: 1.1328125, time: 23:40:50, step: 52\nloss: 0.95703125, time: 23:40:53, step: 53\nloss: 1.3125, time: 23:40:56, step: 54\nloss: 1.0234375, time: 23:40:58, step: 55\nloss: 0.98046875, time: 23:41:01, step: 56\nloss: 1.015625, time: 23:41:04, step: 57\nEpoch 6 train begin 23:41:07\nloss: 1.2578125, time: 23:41:10, step: 1\nloss: 1.0, time: 23:41:14, step: 2\nloss: 0.97265625, time: 23:41:17, step: 3\nloss: 1.234375, time: 23:41:19, step: 4\nloss: 0.88671875, time: 23:41:24, step: 5\nloss: 1.0234375, time: 23:41:26, step: 6\nloss: 0.90625, time: 23:41:29, step: 7\nloss: 1.046875, time: 23:41:32, step: 8\nloss: 1.390625, time: 23:41:36, step: 9\nloss: 0.765625, time: 23:41:39, step: 10\nloss: 1.1484375, time: 23:41:41, step: 11\nloss: 1.3046875, time: 23:41:44, step: 12\nloss: 1.125, time: 23:41:47, step: 13\nloss: 0.99609375, time: 23:41:50, step: 14\nloss: 0.515625, time: 23:41:52, step: 15\nloss: 1.234375, time: 23:41:55, step: 16\nloss: 0.9296875, time: 23:41:58, step: 17\nloss: 0.69921875, time: 23:42:00, step: 18\nloss: 0.99609375, time: 23:42:03, step: 19\nloss: 1.1953125, time: 23:42:06, step: 20\n----- Time -> 23:42:09 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3281\n----- Time -> 23:42:10 ----- Validation Batch -> 2 ----  Validation Loss -> 0.6094\n----- Time -> 23:42:10 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2109\n----- Time -> 23:42:11 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8594\n----- Time -> 23:42:11 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0938\n----- Time -> 23:42:12 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0203125\nloss: 1.09375, time: 23:42:13, step: 21\nloss: 0.92578125, time: 23:42:15, step: 22\nloss: 0.88671875, time: 23:42:18, step: 23\nloss: 0.890625, time: 23:42:20, step: 24\nloss: 1.0625, time: 23:42:23, step: 25\nloss: 1.1875, time: 23:42:26, step: 26\nloss: 0.9609375, time: 23:42:29, step: 27\nloss: 0.96484375, time: 23:42:31, step: 28\nloss: 1.1953125, time: 23:42:34, step: 29\nloss: 0.91796875, time: 23:42:37, step: 30\nloss: 0.8515625, time: 23:42:39, step: 31\nloss: 1.03125, time: 23:42:42, step: 32\nloss: 1.171875, time: 23:42:45, step: 33\nloss: 1.109375, time: 23:42:48, step: 34\nloss: 0.86328125, time: 23:42:50, step: 35\nloss: 1.0625, time: 23:42:53, step: 36\nloss: 0.91015625, time: 23:42:56, step: 37\nloss: 1.2265625, time: 23:42:59, step: 38\nloss: 0.8046875, time: 23:43:01, step: 39\nloss: 0.8515625, time: 23:43:04, step: 40\n----- Time -> 23:43:07 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3281\n----- Time -> 23:43:07 ----- Validation Batch -> 2 ----  Validation Loss -> 0.6055\n----- Time -> 23:43:08 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1953\n----- Time -> 23:43:09 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8555\n----- Time -> 23:43:09 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0859\n----- Time -> 23:43:09 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0140625\nloss: 1.03125, time: 23:43:10, step: 41\nloss: 1.0078125, time: 23:43:13, step: 42\nloss: 1.265625, time: 23:43:15, step: 43\nloss: 0.51953125, time: 23:43:18, step: 44\nloss: 0.7890625, time: 23:43:21, step: 45\nloss: 0.91796875, time: 23:43:24, step: 46\nloss: 0.828125, time: 23:43:26, step: 47\nloss: 0.89453125, time: 23:43:29, step: 48\nloss: 1.0, time: 23:43:32, step: 49\nloss: 1.3359375, time: 23:43:35, step: 50\nloss: 0.98828125, time: 23:43:37, step: 51\nloss: 1.109375, time: 23:43:40, step: 52\nloss: 0.94140625, time: 23:43:43, step: 53\nloss: 1.2890625, time: 23:43:46, step: 54\nloss: 1.0, time: 23:43:48, step: 55\nloss: 0.96875, time: 23:43:51, step: 56\nloss: 0.9921875, time: 23:43:54, step: 57\nEpoch 7 train begin 23:43:57\nloss: 1.234375, time: 23:44:00, step: 1\nloss: 0.98046875, time: 23:44:04, step: 2\nloss: 0.95703125, time: 23:44:07, step: 3\nloss: 1.21875, time: 23:44:09, step: 4\nloss: 0.87109375, time: 23:44:14, step: 5\nloss: 1.0078125, time: 23:44:17, step: 6\nloss: 0.88671875, time: 23:44:19, step: 7\nloss: 1.03125, time: 23:44:22, step: 8\nloss: 1.375, time: 23:44:26, step: 9\nloss: 0.75390625, time: 23:44:29, step: 10\nloss: 1.125, time: 23:44:32, step: 11\nloss: 1.28125, time: 23:44:34, step: 12\nloss: 1.1015625, time: 23:44:37, step: 13\nloss: 0.97265625, time: 23:44:40, step: 14\nloss: 0.5078125, time: 23:44:42, step: 15\nloss: 1.21875, time: 23:44:45, step: 16\nloss: 0.9140625, time: 23:44:48, step: 17\nloss: 0.6875, time: 23:44:51, step: 18\nloss: 0.984375, time: 23:44:53, step: 19\nloss: 1.1796875, time: 23:44:56, step: 20\n----- Time -> 23:44:59 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3203\n----- Time -> 23:45:00 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5977\n----- Time -> 23:45:01 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1797\n----- Time -> 23:45:01 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8438\n----- Time -> 23:45:02 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0781\n----- Time -> 23:45:02 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0039062\nloss: 1.0703125, time: 23:45:03, step: 21\nloss: 0.91015625, time: 23:45:05, step: 22\nloss: 0.87109375, time: 23:45:08, step: 23\nloss: 0.87890625, time: 23:45:11, step: 24\nloss: 1.046875, time: 23:45:14, step: 25\nloss: 1.171875, time: 23:45:16, step: 26\nloss: 0.9453125, time: 23:45:19, step: 27\nloss: 0.953125, time: 23:45:21, step: 28\nloss: 1.1875, time: 23:45:25, step: 29\nloss: 0.90625, time: 23:45:27, step: 30\nloss: 0.8359375, time: 23:45:30, step: 31\nloss: 1.015625, time: 23:45:32, step: 32\nloss: 1.15625, time: 23:45:36, step: 33\nloss: 1.09375, time: 23:45:38, step: 34\nloss: 0.85546875, time: 23:45:41, step: 35\nloss: 1.046875, time: 23:45:43, step: 36\nloss: 0.8984375, time: 23:45:46, step: 37\nloss: 1.2109375, time: 23:45:49, step: 38\nloss: 0.79296875, time: 23:45:52, step: 39\nloss: 0.84375, time: 23:45:54, step: 40\n----- Time -> 23:45:57 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3203\n----- Time -> 23:45:58 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5977\n----- Time -> 23:45:58 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1797\n----- Time -> 23:45:59 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8398\n----- Time -> 23:45:59 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0703\n----- Time -> 23:45:59 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0015625\nloss: 1.0234375, time: 23:46:01, step: 41\nloss: 0.98828125, time: 23:46:03, step: 42\nloss: 1.2578125, time: 23:46:06, step: 43\nloss: 0.51171875, time: 23:46:08, step: 44\nloss: 0.77734375, time: 23:46:11, step: 45\nloss: 0.90625, time: 23:46:14, step: 46\nloss: 0.81640625, time: 23:46:17, step: 47\nloss: 0.88671875, time: 23:46:19, step: 48\nloss: 0.984375, time: 23:46:22, step: 49\nloss: 1.328125, time: 23:46:25, step: 50\nloss: 0.9765625, time: 23:46:28, step: 51\nloss: 1.1015625, time: 23:46:30, step: 52\nloss: 0.9296875, time: 23:46:33, step: 53\nloss: 1.28125, time: 23:46:36, step: 54\nloss: 0.9921875, time: 23:46:38, step: 55\nloss: 0.9609375, time: 23:46:41, step: 56\nloss: 0.97265625, time: 23:46:45, step: 57\nEpoch 8 train begin 23:46:47\nloss: 1.2265625, time: 23:46:50, step: 1\nloss: 0.97265625, time: 23:46:55, step: 2\nloss: 0.94921875, time: 23:46:57, step: 3\nloss: 1.21875, time: 23:47:00, step: 4\nloss: 0.86328125, time: 23:47:05, step: 5\nloss: 0.99609375, time: 23:47:07, step: 6\nloss: 0.87109375, time: 23:47:10, step: 7\nloss: 1.015625, time: 23:47:13, step: 8\nloss: 1.3671875, time: 23:47:17, step: 9\nloss: 0.7421875, time: 23:47:19, step: 10\nloss: 1.1171875, time: 23:47:22, step: 11\nloss: 1.2734375, time: 23:47:25, step: 12\nloss: 1.09375, time: 23:47:28, step: 13\nloss: 0.95703125, time: 23:47:30, step: 14\nloss: 0.50390625, time: 23:47:33, step: 15\nloss: 1.2109375, time: 23:47:36, step: 16\nloss: 0.90625, time: 23:47:39, step: 17\nloss: 0.6796875, time: 23:47:41, step: 18\nloss: 0.9765625, time: 23:47:44, step: 19\nloss: 1.1640625, time: 23:47:46, step: 20\n----- Time -> 23:47:50 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3125\n----- Time -> 23:47:50 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5938\n----- Time -> 23:47:51 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1641\n----- Time -> 23:47:52 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8359\n----- Time -> 23:47:52 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0625\n----- Time -> 23:47:52 ----- Validation Batch Size -> 5 ----  Validation Loss -> 0.9937500\nloss: 1.0546875, time: 23:47:53, step: 21\nloss: 0.90234375, time: 23:47:56, step: 22\nloss: 0.859375, time: 23:47:59, step: 23\nloss: 0.875, time: 23:48:01, step: 24\nloss: 1.0390625, time: 23:48:04, step: 25\nloss: 1.15625, time: 23:48:07, step: 26\nloss: 0.9375, time: 23:48:10, step: 27\nloss: 0.94921875, time: 23:48:12, step: 28\nloss: 1.1796875, time: 23:48:15, step: 29\nloss: 0.8984375, time: 23:48:18, step: 30\nloss: 0.828125, time: 23:48:20, step: 31\nloss: 1.0078125, time: 23:48:23, step: 32\nloss: 1.1484375, time: 23:48:26, step: 33\nloss: 1.0859375, time: 23:48:29, step: 34\nloss: 0.84765625, time: 23:48:31, step: 35\nloss: 1.0390625, time: 23:48:34, step: 36\nloss: 0.890625, time: 23:48:37, step: 37\nloss: 1.203125, time: 23:48:39, step: 38\nloss: 0.78125, time: 23:48:42, step: 39\nloss: 0.8359375, time: 23:48:45, step: 40\n----- Time -> 23:48:47 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3125\n----- Time -> 23:48:48 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5938\n----- Time -> 23:48:49 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1641\n----- Time -> 23:48:49 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8320\n----- Time -> 23:48:50 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0625\n----- Time -> 23:48:50 ----- Validation Batch Size -> 5 ----  Validation Loss -> 0.9929687\nloss: 1.015625, time: 23:48:51, step: 41\nloss: 0.98046875, time: 23:48:54, step: 42\nloss: 1.25, time: 23:48:56, step: 43\nloss: 0.5078125, time: 23:48:59, step: 44\nloss: 0.76953125, time: 23:49:02, step: 45\nloss: 0.8984375, time: 23:49:04, step: 46\nloss: 0.80859375, time: 23:49:07, step: 47\nloss: 0.8828125, time: 23:49:10, step: 48\nloss: 0.9765625, time: 23:49:13, step: 49\nloss: 1.3203125, time: 23:49:15, step: 50\nloss: 0.96875, time: 23:49:18, step: 51\nloss: 1.0859375, time: 23:49:21, step: 52\nloss: 0.92578125, time: 23:49:24, step: 53\nloss: 1.2734375, time: 23:49:26, step: 54\nloss: 0.984375, time: 23:49:29, step: 55\nloss: 0.953125, time: 23:49:31, step: 56\nloss: 0.9609375, time: 23:49:35, step: 57\nEpoch 9 train begin 23:49:37\nloss: 1.21875, time: 23:49:40, step: 1\nloss: 0.96484375, time: 23:49:45, step: 2\nloss: 0.94140625, time: 23:49:47, step: 3\nloss: 1.2109375, time: 23:49:50, step: 4\nloss: 0.859375, time: 23:49:55, step: 5\nloss: 0.9921875, time: 23:49:57, step: 6\nloss: 0.86328125, time: 23:50:00, step: 7\nloss: 1.015625, time: 23:50:03, step: 8\nloss: 1.359375, time: 23:50:07, step: 9\nloss: 0.73828125, time: 23:50:10, step: 10\nloss: 1.109375, time: 23:50:12, step: 11\nloss: 1.265625, time: 23:50:15, step: 12\nloss: 1.0859375, time: 23:50:18, step: 13\nloss: 0.94921875, time: 23:50:21, step: 14\nloss: 0.5, time: 23:50:23, step: 15\nloss: 1.203125, time: 23:50:26, step: 16\nloss: 0.8984375, time: 23:50:29, step: 17\nloss: 0.67578125, time: 23:50:31, step: 18\nloss: 0.97265625, time: 23:50:34, step: 19\nloss: 1.15625, time: 23:50:37, step: 20\n----- Time -> 23:50:40 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3125\n----- Time -> 23:50:41 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5898\n----- Time -> 23:50:41 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1641\n----- Time -> 23:50:42 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8320\n----- Time -> 23:50:43 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0625\n----- Time -> 23:50:43 ----- Validation Batch Size -> 5 ----  Validation Loss -> 0.9921875\nloss: 1.0546875, time: 23:50:44, step: 21\nloss: 0.89453125, time: 23:50:46, step: 22\nloss: 0.85546875, time: 23:50:49, step: 23\nloss: 0.8671875, time: 23:50:51, step: 24\nloss: 1.03125, time: 23:50:54, step: 25\nloss: 1.15625, time: 23:50:57, step: 26\nloss: 0.93359375, time: 23:51:00, step: 27\nloss: 0.94921875, time: 23:51:02, step: 28\nloss: 1.171875, time: 23:51:05, step: 29\nloss: 0.89453125, time: 23:51:08, step: 30\nloss: 0.82421875, time: 23:51:10, step: 31\nloss: 1.0078125, time: 23:51:13, step: 32\nloss: 1.1484375, time: 23:51:16, step: 33\nloss: 1.078125, time: 23:51:19, step: 34\nloss: 0.84375, time: 23:51:21, step: 35\nloss: 1.0390625, time: 23:51:24, step: 36\nloss: 0.890625, time: 23:51:27, step: 37\nloss: 1.203125, time: 23:51:30, step: 38\nloss: 0.77734375, time: 23:51:32, step: 39\nloss: 0.83203125, time: 23:51:35, step: 40\n----- Time -> 23:51:38 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3125\n----- Time -> 23:51:38 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5898\n----- Time -> 23:51:39 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1562\n----- Time -> 23:51:40 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8281\n----- Time -> 23:51:40 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0547\n----- Time -> 23:51:40 ----- Validation Batch Size -> 5 ----  Validation Loss -> 0.9882812\nloss: 1.015625, time: 23:51:41, step: 41\nloss: 0.97265625, time: 23:51:44, step: 42\nloss: 1.25, time: 23:51:46, step: 43\nloss: 0.50390625, time: 23:51:49, step: 44\nloss: 0.765625, time: 23:51:52, step: 45\nloss: 0.89453125, time: 23:51:55, step: 46\nloss: 0.80859375, time: 23:51:57, step: 47\nloss: 0.87890625, time: 23:52:00, step: 48\nloss: 0.9765625, time: 23:52:03, step: 49\nloss: 1.3203125, time: 23:52:06, step: 50\nloss: 0.96875, time: 23:52:08, step: 51\nloss: 1.0859375, time: 23:52:11, step: 52\nloss: 0.921875, time: 23:52:14, step: 53\nloss: 1.265625, time: 23:52:17, step: 54\nloss: 0.98046875, time: 23:52:19, step: 55\nloss: 0.953125, time: 23:52:22, step: 56\nloss: 0.95703125, time: 23:52:25, step: 57\nEpoch 10 train begin 23:52:28\nloss: 1.21875, time: 23:52:30, step: 1\nloss: 0.96484375, time: 23:52:35, step: 2\nloss: 0.94140625, time: 23:52:38, step: 3\nloss: 1.2109375, time: 23:52:40, step: 4\nloss: 0.85546875, time: 23:52:45, step: 5\nloss: 0.98828125, time: 23:52:47, step: 6\nloss: 0.86328125, time: 23:52:50, step: 7\nloss: 1.0078125, time: 23:52:53, step: 8\nloss: 1.359375, time: 23:52:57, step: 9\nloss: 0.73828125, time: 23:53:00, step: 10\nloss: 1.1015625, time: 23:53:03, step: 11\nloss: 1.265625, time: 23:53:05, step: 12\nloss: 1.078125, time: 23:53:08, step: 13\nloss: 0.94921875, time: 23:53:11, step: 14\nloss: 0.5, time: 23:53:13, step: 15\nloss: 1.203125, time: 23:53:16, step: 16\nloss: 0.8984375, time: 23:53:19, step: 17\nloss: 0.67578125, time: 23:53:22, step: 18\nloss: 0.97265625, time: 23:53:24, step: 19\nloss: 1.15625, time: 23:53:27, step: 20\n----- Time -> 23:53:30 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3125\n----- Time -> 23:53:31 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5898\n----- Time -> 23:53:31 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1562\n----- Time -> 23:53:32 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8281\n----- Time -> 23:53:33 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0625\n----- Time -> 23:53:33 ----- Validation Batch Size -> 5 ----  Validation Loss -> 0.9898438\nloss: 1.046875, time: 23:53:34, step: 21\nloss: 0.89453125, time: 23:53:36, step: 22\nloss: 0.85546875, time: 23:53:39, step: 23\nloss: 0.8671875, time: 23:53:41, step: 24\nloss: 1.03125, time: 23:55:53, step: 25\nloss: 1.1484375, time: 23:55:56, step: 26\nloss: 0.9296875, time: 23:55:58, step: 27\nloss: 0.9453125, time: 23:56:01, step: 28\nloss: 1.171875, time: 23:56:04, step: 29\nloss: 0.89453125, time: 23:56:07, step: 30\nloss: 0.82421875, time: 23:56:09, step: 31\nloss: 1.0078125, time: 23:56:12, step: 32\nloss: 1.1484375, time: 23:56:15, step: 33\nloss: 1.078125, time: 23:56:18, step: 34\nloss: 0.84375, time: 23:56:20, step: 35\nloss: 1.0390625, time: 23:56:23, step: 36\nloss: 0.88671875, time: 23:56:26, step: 37\nloss: 1.203125, time: 23:56:28, step: 38\nloss: 0.77734375, time: 23:56:31, step: 39\nloss: 0.83203125, time: 23:56:33, step: 40\n----- Time -> 23:56:36 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3125\n----- Time -> 23:56:37 ----- Validation Batch -> 2 ----  Validation Loss -> 0.5898\n----- Time -> 23:56:37 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1562\n----- Time -> 23:56:38 ----- Validation Batch -> 4 ----  Validation Loss -> 0.8281\n----- Time -> 23:56:39 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0547\n----- Time -> 23:56:39 ----- Validation Batch Size -> 5 ----  Validation Loss -> 0.9882812\nloss: 1.0078125, time: 23:56:40, step: 41\nloss: 0.97265625, time: 23:56:42, step: 42\nloss: 1.25, time: 23:56:45, step: 43\nloss: 0.50390625, time: 23:56:48, step: 44\nloss: 0.765625, time: 23:56:51, step: 45\nloss: 0.89453125, time: 23:56:53, step: 46\nloss: 0.80859375, time: 23:56:56, step: 47\nloss: 0.87890625, time: 23:56:58, step: 48\nloss: 0.97265625, time: 23:57:02, step: 49\nloss: 1.3203125, time: 23:57:04, step: 50\nloss: 0.96875, time: 23:57:07, step: 51\nloss: 1.0859375, time: 23:57:09, step: 52\nloss: 0.921875, time: 23:57:12, step: 53\nloss: 1.265625, time: 23:57:15, step: 54\nloss: 0.98046875, time: 23:57:17, step: 55\nloss: 0.953125, time: 23:57:20, step: 56\nloss: 0.95703125, time: 23:57:23, step: 57\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>▁█████████▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>Validation_loss</td><td>▃█▇█▇▇▅▆▇▂▇▇▆▂▆▆▅▁▆▃▅▆▁▃▅▄▆▁▃▆▅▁▅▆▁▄▆▁▁▃</td></tr><tr><td>train_loss</td><td>▆█▇▇▅▆▆▅▆▆▆▇▂▆▄▅▄▁▇▄▅▅▃▆▇▄▄▄▅▄▆▆▆▅▃▃▆▅▅▁</td></tr><tr><td>train_step</td><td>▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>val_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>0.0</td></tr><tr><td>Validation_loss</td><td>1.05469</td></tr><tr><td>train_loss</td><td>0.95703</td></tr><tr><td>train_step</td><td>570</td></tr><tr><td>val_step</td><td>100</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">snowy-shape-6</strong> at: <a href='https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R/runs/z2g0lsdt' target=\"_blank\">https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R/runs/z2g0lsdt</a><br> View project at: <a href='https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R' target=\"_blank\">https://wandb.ai/shoann-mycompany-inc/Llama-SEA-LION-v3.5-8B-R</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250513_231100-z2g0lsdt/logs</code>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import time\nprint('Loading the model on CPU')\nSTART=time.time()\nmodel = model.cpu()\nprint(f\"Loaded model on cpu in {time.time()-START} seconds \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T23:57:27.400897Z","iopub.execute_input":"2025-05-13T23:57:27.401546Z","iopub.status.idle":"2025-05-13T23:59:38.156891Z","shell.execute_reply.started":"2025-05-13T23:57:27.401516Z","shell.execute_reply":"2025-05-13T23:59:38.151057Z"}},"outputs":[{"name":"stdout","text":"Loading the model on CPU\nLoaded model on cpu in 130.74386429786682 seconds \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from huggingface_hub import login\n\nsecret_label = \"HF_TOKEN\"\nlogin(UserSecretsClient().get_secret(secret_label))\n\nmodel.push_to_hub(\n    SAVED_MODEL, \n    tokenizer=tokenizer,\n    # safe_serialization=True,\n    # create_pr=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:19:27.239790Z","iopub.execute_input":"2025-05-14T00:19:27.240169Z","iopub.status.idle":"2025-05-14T00:19:30.300500Z","shell.execute_reply.started":"2025-05-14T00:19:27.240122Z","shell.execute_reply":"2025-05-14T00:19:30.294290Z"}},"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ShoAnn/Llama-SEA-LION-v3.5-8B-R-legalqa/commit/0cd54bc2862d7f54d5f6d23e22a8dead3d8482a8', commit_message='Upload model', commit_description='', oid='0cd54bc2862d7f54d5f6d23e22a8dead3d8482a8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ShoAnn/Llama-SEA-LION-v3.5-8B-R-legalqa', endpoint='https://huggingface.co', repo_type='model', repo_id='ShoAnn/Llama-SEA-LION-v3.5-8B-R-legalqa'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"!pip install -U transformers accelerate peft huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:19:03.711321Z","iopub.execute_input":"2025-05-14T00:19:03.711711Z","iopub.status.idle":"2025-05-14T00:19:10.689683Z","shell.execute_reply.started":"2025-05-14T00:19:03.711681Z","shell.execute_reply":"2025-05-14T00:19:10.683372Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.51.3)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/site-packages (1.6.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/site-packages (0.15.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/site-packages (0.31.1)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.2/484.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.5.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers) (0.21.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2.0.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.11.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (2.6.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate) (7.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface_hub) (4.13.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface_hub) (2025.3.0)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.10)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nInstalling collected packages: huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.31.1\n    Uninstalling huggingface-hub-0.31.1:\n      Successfully uninstalled huggingface-hub-0.31.1\nSuccessfully installed huggingface_hub-0.31.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:16:51.865680Z","iopub.execute_input":"2025-05-14T00:16:51.866051Z","iopub.status.idle":"2025-05-14T00:16:51.891932Z","shell.execute_reply.started":"2025-05-14T00:16:51.866021Z","shell.execute_reply":"2025-05-14T00:16:51.888152Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}